{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4615a83d-b9cd-4c1d-84df-d5e75c894c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:01.387894429Z",
     "start_time": "2023-05-23T15:01:01.366280485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alberto/Work/course_interpretability_deep_learning\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a073f0-ba9d-4545-a0f4-a49d638b7695",
   "metadata": {},
   "source": [
    "# Multi-omics stratification on PDAC patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7dffc4-d560-47bb-84b8-a6870d4824a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:03.420757620Z",
     "start_time": "2023-05-23T15:01:03.416255778Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import optuna\n",
    "import time\n",
    "import dill\n",
    "import shutil\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "from src import settings\n",
    "from explainability import FeatureImportance\n",
    "from utils import transform_full_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991e212-7498-4da5-b089-c355594b1113",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525e21f1-eb98-424d-91e1-328329b4e786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:09.194541257Z",
     "start_time": "2023-05-23T15:01:05.607110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "methylation_data.shape (153, 301195)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg00000029</th>\n",
       "      <th>cg00000236</th>\n",
       "      <th>cg00000289</th>\n",
       "      <th>cg00000292</th>\n",
       "      <th>cg00000321</th>\n",
       "      <th>cg00000622</th>\n",
       "      <th>cg00000658</th>\n",
       "      <th>cg00000714</th>\n",
       "      <th>cg00000721</th>\n",
       "      <th>cg00000734</th>\n",
       "      <th>...</th>\n",
       "      <th>ch.9.2262725R</th>\n",
       "      <th>ch.9.2285199R</th>\n",
       "      <th>ch.9.2298007R</th>\n",
       "      <th>ch.9.2473665R</th>\n",
       "      <th>ch.9.357218F</th>\n",
       "      <th>ch.9.377428R</th>\n",
       "      <th>ch.9.691424R</th>\n",
       "      <th>ch.9.837340R</th>\n",
       "      <th>ch.9.898515R</th>\n",
       "      <th>ch.9.991104F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB6</th>\n",
       "      <td>0.157951</td>\n",
       "      <td>0.836226</td>\n",
       "      <td>0.710511</td>\n",
       "      <td>0.560780</td>\n",
       "      <td>0.239194</td>\n",
       "      <td>0.016433</td>\n",
       "      <td>0.864604</td>\n",
       "      <td>0.087681</td>\n",
       "      <td>0.938775</td>\n",
       "      <td>0.061008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103136</td>\n",
       "      <td>0.053757</td>\n",
       "      <td>0.032478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.064965</td>\n",
       "      <td>0.049776</td>\n",
       "      <td>0.115268</td>\n",
       "      <td>0.095954</td>\n",
       "      <td>0.084203</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB8</th>\n",
       "      <td>0.300754</td>\n",
       "      <td>0.782242</td>\n",
       "      <td>0.574296</td>\n",
       "      <td>0.670286</td>\n",
       "      <td>0.424310</td>\n",
       "      <td>0.014747</td>\n",
       "      <td>0.885958</td>\n",
       "      <td>0.112524</td>\n",
       "      <td>0.930765</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028180</td>\n",
       "      <td>0.054483</td>\n",
       "      <td>0.022736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.060835</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.160082</td>\n",
       "      <td>0.059216</td>\n",
       "      <td>0.065342</td>\n",
       "      <td>0.166304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB9</th>\n",
       "      <td>0.257807</td>\n",
       "      <td>0.846522</td>\n",
       "      <td>0.534748</td>\n",
       "      <td>0.688073</td>\n",
       "      <td>0.295597</td>\n",
       "      <td>0.014649</td>\n",
       "      <td>0.895039</td>\n",
       "      <td>0.167297</td>\n",
       "      <td>0.940112</td>\n",
       "      <td>0.058407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059313</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.032581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055342</td>\n",
       "      <td>0.069086</td>\n",
       "      <td>0.128546</td>\n",
       "      <td>0.120015</td>\n",
       "      <td>0.074940</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABA</th>\n",
       "      <td>0.239086</td>\n",
       "      <td>0.789457</td>\n",
       "      <td>0.474723</td>\n",
       "      <td>0.705372</td>\n",
       "      <td>0.530321</td>\n",
       "      <td>0.016919</td>\n",
       "      <td>0.884874</td>\n",
       "      <td>0.129581</td>\n",
       "      <td>0.910885</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122677</td>\n",
       "      <td>0.056068</td>\n",
       "      <td>0.023190</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>0.082979</td>\n",
       "      <td>0.057172</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.121676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABE</th>\n",
       "      <td>0.168622</td>\n",
       "      <td>0.841684</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>0.623799</td>\n",
       "      <td>0.322576</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>0.898202</td>\n",
       "      <td>0.125415</td>\n",
       "      <td>0.941153</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046699</td>\n",
       "      <td>0.049177</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075854</td>\n",
       "      <td>0.062602</td>\n",
       "      <td>0.122072</td>\n",
       "      <td>0.082753</td>\n",
       "      <td>0.071240</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              cg00000029  cg00000236  cg00000289  cg00000292  cg00000321   \n",
       "TCGA-2J-AAB6    0.157951    0.836226    0.710511    0.560780    0.239194  \\\n",
       "TCGA-2J-AAB8    0.300754    0.782242    0.574296    0.670286    0.424310   \n",
       "TCGA-2J-AAB9    0.257807    0.846522    0.534748    0.688073    0.295597   \n",
       "TCGA-2J-AABA    0.239086    0.789457    0.474723    0.705372    0.530321   \n",
       "TCGA-2J-AABE    0.168622    0.841684    0.591205    0.623799    0.322576   \n",
       "\n",
       "              cg00000622  cg00000658  cg00000714  cg00000721  cg00000734  ...   \n",
       "TCGA-2J-AAB6    0.016433    0.864604    0.087681    0.938775    0.061008  ...  \\\n",
       "TCGA-2J-AAB8    0.014747    0.885958    0.112524    0.930765    0.037198  ...   \n",
       "TCGA-2J-AAB9    0.014649    0.895039    0.167297    0.940112    0.058407  ...   \n",
       "TCGA-2J-AABA    0.016919    0.884874    0.129581    0.910885    0.062167  ...   \n",
       "TCGA-2J-AABE    0.014408    0.898202    0.125415    0.941153    0.059365  ...   \n",
       "\n",
       "              ch.9.2262725R  ch.9.2285199R  ch.9.2298007R  ch.9.2473665R   \n",
       "TCGA-2J-AAB6       0.103136       0.053757       0.032478            NaN  \\\n",
       "TCGA-2J-AAB8       0.028180       0.054483       0.022736            NaN   \n",
       "TCGA-2J-AAB9       0.059313       0.063187       0.032581            NaN   \n",
       "TCGA-2J-AABA       0.122677       0.056068       0.023190       0.109351   \n",
       "TCGA-2J-AABE       0.046699       0.049177       0.032707            NaN   \n",
       "\n",
       "              ch.9.357218F  ch.9.377428R  ch.9.691424R  ch.9.837340R   \n",
       "TCGA-2J-AAB6      0.064965      0.049776      0.115268      0.095954  \\\n",
       "TCGA-2J-AAB8      0.060835      0.036434      0.160082      0.059216   \n",
       "TCGA-2J-AAB9      0.055342      0.069086      0.128546      0.120015   \n",
       "TCGA-2J-AABA      0.056015      0.053238      0.082979      0.057172   \n",
       "TCGA-2J-AABE      0.075854      0.062602      0.122072      0.082753   \n",
       "\n",
       "              ch.9.898515R  ch.9.991104F  \n",
       "TCGA-2J-AAB6      0.084203           NaN  \n",
       "TCGA-2J-AAB8      0.065342      0.166304  \n",
       "TCGA-2J-AAB9      0.074940           NaN  \n",
       "TCGA-2J-AABA      0.045781      0.121676  \n",
       "TCGA-2J-AABE      0.071240           NaN  \n",
       "\n",
       "[5 rows x 301195 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methylation_data = pd.read_csv(settings.methylation_data_path, sep=\";\", index_col=0, decimal=\",\")\n",
    "methylation_data.columns = methylation_data.columns.str.replace(\".\", \"-\")\n",
    "methylation_data = methylation_data.T\n",
    "methylation_data = methylation_data.astype(np.float32)\n",
    "print(\"methylation_data.shape\", methylation_data.shape)\n",
    "methylation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba9fa3a-ba17-49a1-86ef-6b399dfaafed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:09.359360346Z",
     "start_time": "2023-05-23T15:01:09.194876421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnaseq_data.shape (147, 20501)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2BP1</th>\n",
       "      <th>A2LD1</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>A4GNT</th>\n",
       "      <th>AAA1</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>...</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>psiTPTE22</th>\n",
       "      <th>tAKR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB6</th>\n",
       "      <td>82.549698</td>\n",
       "      <td>8.187100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>163.122803</td>\n",
       "      <td>1815.789551</td>\n",
       "      <td>8517.444336</td>\n",
       "      <td>1121.052612</td>\n",
       "      <td>1.169600</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>834.502930</td>\n",
       "      <td>...</td>\n",
       "      <td>14.619900</td>\n",
       "      <td>269.005798</td>\n",
       "      <td>1053.216431</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>683.625671</td>\n",
       "      <td>11696.491211</td>\n",
       "      <td>869.005798</td>\n",
       "      <td>601.754395</td>\n",
       "      <td>26.315800</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB8</th>\n",
       "      <td>56.930698</td>\n",
       "      <td>33.842499</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>185.814301</td>\n",
       "      <td>16.921301</td>\n",
       "      <td>14413.913086</td>\n",
       "      <td>392.949493</td>\n",
       "      <td>9.400700</td>\n",
       "      <td>0.9401</td>\n",
       "      <td>801.880127</td>\n",
       "      <td>...</td>\n",
       "      <td>35.722698</td>\n",
       "      <td>356.286713</td>\n",
       "      <td>829.142212</td>\n",
       "      <td>3.7603</td>\n",
       "      <td>680.611023</td>\n",
       "      <td>5829.377441</td>\n",
       "      <td>828.202087</td>\n",
       "      <td>609.165710</td>\n",
       "      <td>85.546402</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB9</th>\n",
       "      <td>105.787804</td>\n",
       "      <td>21.436199</td>\n",
       "      <td>1.0718</td>\n",
       "      <td>166.709503</td>\n",
       "      <td>642.015015</td>\n",
       "      <td>24311.779297</td>\n",
       "      <td>1125.401855</td>\n",
       "      <td>50.375099</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>862.808105</td>\n",
       "      <td>...</td>\n",
       "      <td>57.877800</td>\n",
       "      <td>381.564789</td>\n",
       "      <td>936.763123</td>\n",
       "      <td>1.0718</td>\n",
       "      <td>646.302307</td>\n",
       "      <td>8094.319336</td>\n",
       "      <td>1083.601318</td>\n",
       "      <td>573.419128</td>\n",
       "      <td>30.010700</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABA</th>\n",
       "      <td>99.345497</td>\n",
       "      <td>18.788200</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>99.276703</td>\n",
       "      <td>873.649597</td>\n",
       "      <td>10302.006836</td>\n",
       "      <td>633.161072</td>\n",
       "      <td>6.262700</td>\n",
       "      <td>18.7882</td>\n",
       "      <td>623.767029</td>\n",
       "      <td>...</td>\n",
       "      <td>52.606899</td>\n",
       "      <td>293.721588</td>\n",
       "      <td>1511.820923</td>\n",
       "      <td>1.2525</td>\n",
       "      <td>945.670898</td>\n",
       "      <td>4829.810547</td>\n",
       "      <td>1364.646851</td>\n",
       "      <td>793.486816</td>\n",
       "      <td>31.313601</td>\n",
       "      <td>0.6263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABE</th>\n",
       "      <td>79.401901</td>\n",
       "      <td>3.083100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>134.564499</td>\n",
       "      <td>74.610802</td>\n",
       "      <td>11076.861328</td>\n",
       "      <td>710.343811</td>\n",
       "      <td>35.147202</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>702.327698</td>\n",
       "      <td>...</td>\n",
       "      <td>56.728802</td>\n",
       "      <td>431.632507</td>\n",
       "      <td>1069.215454</td>\n",
       "      <td>0.6166</td>\n",
       "      <td>564.205322</td>\n",
       "      <td>7464.775879</td>\n",
       "      <td>832.434082</td>\n",
       "      <td>468.629608</td>\n",
       "      <td>48.096199</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    A1BG       A1CF   A2BP1       A2LD1        A2ML1   \n",
       "TCGA-2J-AAB6   82.549698   8.187100  0.0000  163.122803  1815.789551  \\\n",
       "TCGA-2J-AAB8   56.930698  33.842499  0.0000  185.814301    16.921301   \n",
       "TCGA-2J-AAB9  105.787804  21.436199  1.0718  166.709503   642.015015   \n",
       "TCGA-2J-AABA   99.345497  18.788200  0.0000   99.276703   873.649597   \n",
       "TCGA-2J-AABE   79.401901   3.083100  0.0000  134.564499    74.610802   \n",
       "\n",
       "                       A2M       A4GALT      A4GNT     AAA1        AAAS  ...   \n",
       "TCGA-2J-AAB6   8517.444336  1121.052612   1.169600   1.1696  834.502930  ...  \\\n",
       "TCGA-2J-AAB8  14413.913086   392.949493   9.400700   0.9401  801.880127  ...   \n",
       "TCGA-2J-AAB9  24311.779297  1125.401855  50.375099   0.0000  862.808105  ...   \n",
       "TCGA-2J-AABA  10302.006836   633.161072   6.262700  18.7882  623.767029  ...   \n",
       "TCGA-2J-AABE  11076.861328   710.343811  35.147202   0.0000  702.327698  ...   \n",
       "\n",
       "                   ZXDA        ZXDB         ZXDC  ZYG11A      ZYG11B   \n",
       "TCGA-2J-AAB6  14.619900  269.005798  1053.216431  0.5848  683.625671  \\\n",
       "TCGA-2J-AAB8  35.722698  356.286713   829.142212  3.7603  680.611023   \n",
       "TCGA-2J-AAB9  57.877800  381.564789   936.763123  1.0718  646.302307   \n",
       "TCGA-2J-AABA  52.606899  293.721588  1511.820923  1.2525  945.670898   \n",
       "TCGA-2J-AABE  56.728802  431.632507  1069.215454  0.6166  564.205322   \n",
       "\n",
       "                       ZYX        ZZEF1        ZZZ3  psiTPTE22    tAKR  \n",
       "TCGA-2J-AAB6  11696.491211   869.005798  601.754395  26.315800  0.0000  \n",
       "TCGA-2J-AAB8   5829.377441   828.202087  609.165710  85.546402  0.0000  \n",
       "TCGA-2J-AAB9   8094.319336  1083.601318  573.419128  30.010700  0.0000  \n",
       "TCGA-2J-AABA   4829.810547  1364.646851  793.486816  31.313601  0.6263  \n",
       "TCGA-2J-AABE   7464.775879   832.434082  468.629608  48.096199  0.0000  \n",
       "\n",
       "[5 rows x 20501 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnaseq_data = pd.read_csv(settings.rnaseq_data_path, sep=\";\", index_col=0, decimal=\",\")\n",
    "rnaseq_data = rnaseq_data.T\n",
    "rnaseq_data = rnaseq_data.astype(np.float32)\n",
    "print(\"rnaseq_data.shape\", rnaseq_data.shape)\n",
    "rnaseq_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f89678-93ce-419e-9d58-386a8d45026f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:11.293572678Z",
     "start_time": "2023-05-23T15:01:11.249405517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common samples: 147\n"
     ]
    }
   ],
   "source": [
    "samples = methylation_data.index.intersection(rnaseq_data.index)\n",
    "methylation_data = methylation_data.loc[samples]\n",
    "rnaseq_data = rnaseq_data.loc[samples]\n",
    "assert methylation_data.index.equals(rnaseq_data.index)\n",
    "Xs= [rnaseq_data, methylation_data]\n",
    "print(\"common samples:\", len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b613bd30-1831-41d6-adb2-e1f8de8cddb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimization_results.shape (506, 56)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>value</th>\n",
       "      <th>datetime_start</th>\n",
       "      <th>datetime_complete</th>\n",
       "      <th>duration</th>\n",
       "      <th>params_divisor_units</th>\n",
       "      <th>params_features_per_component</th>\n",
       "      <th>params_lambda_coeff</th>\n",
       "      <th>params_n_clusters</th>\n",
       "      <th>params_n_epochs</th>\n",
       "      <th>...</th>\n",
       "      <th>user_attrs_val_dist_loss</th>\n",
       "      <th>user_attrs_val_dist_loss_list</th>\n",
       "      <th>user_attrs_val_loss</th>\n",
       "      <th>user_attrs_val_loss_list</th>\n",
       "      <th>user_attrs_val_loss_view_list</th>\n",
       "      <th>user_attrs_val_silhscore</th>\n",
       "      <th>user_attrs_val_silhscore_list</th>\n",
       "      <th>user_attrs_val_total_loss</th>\n",
       "      <th>user_attrs_val_total_loss_list</th>\n",
       "      <th>state</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>501</td>\n",
       "      <td>0.163591</td>\n",
       "      <td>2023-06-27 07:15:08.385178</td>\n",
       "      <td>2023-06-27 07:29:39.886878</td>\n",
       "      <td>0 days 00:14:31.501700</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.179886</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.959966</td>\n",
       "      <td>[27.75557518005371, 78.60169982910156, 20.5489...</td>\n",
       "      <td>0.589241</td>\n",
       "      <td>[0.5634028911590576, 0.634914755821228, 0.5719...</td>\n",
       "      <td>[0.5645923161506653, 0.6385385346412659]</td>\n",
       "      <td>0.163591</td>\n",
       "      <td>[0.13774972, 0.12624748, 0.12726697, 0.1607680...</td>\n",
       "      <td>4.715641</td>\n",
       "      <td>[5.552801609039307, 14.769259452819824, 4.2647...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>502</td>\n",
       "      <td>0.162827</td>\n",
       "      <td>2023-06-27 07:29:41.102885</td>\n",
       "      <td>2023-06-27 07:49:37.924477</td>\n",
       "      <td>0 days 00:19:56.821592</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.210878</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>22.437945</td>\n",
       "      <td>[36.60449981689453, 25.86542510986328, 15.3619...</td>\n",
       "      <td>0.590230</td>\n",
       "      <td>[0.5759981870651245, 0.6293062567710876, 0.573...</td>\n",
       "      <td>[0.5655155801773071, 0.6396599531173706]</td>\n",
       "      <td>0.162827</td>\n",
       "      <td>[0.16576178, 0.16522004, 0.11999136, 0.0996076...</td>\n",
       "      <td>5.318566</td>\n",
       "      <td>[8.296555519104004, 6.0792622566223145, 3.8077...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>505</td>\n",
       "      <td>0.153462</td>\n",
       "      <td>2023-06-27 08:51:00.586990</td>\n",
       "      <td>2023-06-27 09:10:28.564202</td>\n",
       "      <td>0 days 00:19:27.977212</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.163505</td>\n",
       "      <td>[20.011436462402344, 20.025657653808594, 13.80...</td>\n",
       "      <td>0.589781</td>\n",
       "      <td>[0.5678296685218811, 0.6270660758018494, 0.592...</td>\n",
       "      <td>[0.5650894331932068, 0.6391655731201172]</td>\n",
       "      <td>0.153462</td>\n",
       "      <td>[0.1372213, 0.10470337, 0.13370039, 0.1014813,...</td>\n",
       "      <td>5.665669</td>\n",
       "      <td>[4.602327346801758, 4.664762020111084, 3.37358...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20</td>\n",
       "      <td>0.149752</td>\n",
       "      <td>2023-06-26 21:42:18.864873</td>\n",
       "      <td>2023-06-26 21:43:55.222307</td>\n",
       "      <td>0 days 00:01:36.357434</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.081139</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.551882</td>\n",
       "      <td>[14.899971008300781, 13.861332893371582, 14.68...</td>\n",
       "      <td>0.608227</td>\n",
       "      <td>[0.6020955443382263, 0.643612802028656, 0.6064...</td>\n",
       "      <td>[0.5893636441230774, 0.6459534573554992]</td>\n",
       "      <td>0.149752</td>\n",
       "      <td>[0.14876075, 0.12222156, 0.08077106, 0.1501677...</td>\n",
       "      <td>2.190145</td>\n",
       "      <td>[1.800790786743164, 1.7654943466186523, 1.7936...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>503</td>\n",
       "      <td>0.144635</td>\n",
       "      <td>2023-06-27 07:49:39.139816</td>\n",
       "      <td>2023-06-27 08:09:35.080147</td>\n",
       "      <td>0 days 00:19:55.940331</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0.197449</td>\n",
       "      <td>2.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>...</td>\n",
       "      <td>19.137837</td>\n",
       "      <td>[9.212821960449219, 19.08188247680664, 18.7630...</td>\n",
       "      <td>0.589272</td>\n",
       "      <td>[0.5646654367446899, 0.6228009462356567, 0.588...</td>\n",
       "      <td>[0.5639174604415893, 0.6399813556671142]</td>\n",
       "      <td>0.144635</td>\n",
       "      <td>[0.1777916, 0.10118558, 0.13280979, 0.10733721...</td>\n",
       "      <td>4.363973</td>\n",
       "      <td>[2.379190444946289, 4.389517784118652, 4.29368...</td>\n",
       "      <td>COMPLETE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   number     value              datetime_start           datetime_complete   \n",
       "0     501  0.163591  2023-06-27 07:15:08.385178  2023-06-27 07:29:39.886878  \\\n",
       "1     502  0.162827  2023-06-27 07:29:41.102885  2023-06-27 07:49:37.924477   \n",
       "2     505  0.153462  2023-06-27 08:51:00.586990  2023-06-27 09:10:28.564202   \n",
       "3      20  0.149752  2023-06-26 21:42:18.864873  2023-06-26 21:43:55.222307   \n",
       "4     503  0.144635  2023-06-27 07:49:39.139816  2023-06-27 08:09:35.080147   \n",
       "\n",
       "                 duration  params_divisor_units   \n",
       "0  0 days 00:14:31.501700                     2  \\\n",
       "1  0 days 00:19:56.821592                     2   \n",
       "2  0 days 00:19:27.977212                     2   \n",
       "3  0 days 00:01:36.357434                     2   \n",
       "4  0 days 00:19:55.940331                     2   \n",
       "\n",
       "   params_features_per_component  params_lambda_coeff  params_n_clusters   \n",
       "0                              9             0.179886                2.0  \\\n",
       "1                              9             0.210878                2.0   \n",
       "2                              9             0.201835                2.0   \n",
       "3                              5             0.081139                2.0   \n",
       "4                              9             0.197449                2.0   \n",
       "\n",
       "   params_n_epochs  ...  user_attrs_val_dist_loss   \n",
       "0             80.0  ...                 22.959966  \\\n",
       "1             80.0  ...                 22.437945   \n",
       "2             80.0  ...                 25.163505   \n",
       "3             20.0  ...                 19.551882   \n",
       "4             80.0  ...                 19.137837   \n",
       "\n",
       "                       user_attrs_val_dist_loss_list user_attrs_val_loss   \n",
       "0  [27.75557518005371, 78.60169982910156, 20.5489...            0.589241  \\\n",
       "1  [36.60449981689453, 25.86542510986328, 15.3619...            0.590230   \n",
       "2  [20.011436462402344, 20.025657653808594, 13.80...            0.589781   \n",
       "3  [14.899971008300781, 13.861332893371582, 14.68...            0.608227   \n",
       "4  [9.212821960449219, 19.08188247680664, 18.7630...            0.589272   \n",
       "\n",
       "                            user_attrs_val_loss_list   \n",
       "0  [0.5634028911590576, 0.634914755821228, 0.5719...  \\\n",
       "1  [0.5759981870651245, 0.6293062567710876, 0.573...   \n",
       "2  [0.5678296685218811, 0.6270660758018494, 0.592...   \n",
       "3  [0.6020955443382263, 0.643612802028656, 0.6064...   \n",
       "4  [0.5646654367446899, 0.6228009462356567, 0.588...   \n",
       "\n",
       "              user_attrs_val_loss_view_list  user_attrs_val_silhscore   \n",
       "0  [0.5645923161506653, 0.6385385346412659]                  0.163591  \\\n",
       "1  [0.5655155801773071, 0.6396599531173706]                  0.162827   \n",
       "2  [0.5650894331932068, 0.6391655731201172]                  0.153462   \n",
       "3  [0.5893636441230774, 0.6459534573554992]                  0.149752   \n",
       "4  [0.5639174604415893, 0.6399813556671142]                  0.144635   \n",
       "\n",
       "                       user_attrs_val_silhscore_list   \n",
       "0  [0.13774972, 0.12624748, 0.12726697, 0.1607680...  \\\n",
       "1  [0.16576178, 0.16522004, 0.11999136, 0.0996076...   \n",
       "2  [0.1372213, 0.10470337, 0.13370039, 0.1014813,...   \n",
       "3  [0.14876075, 0.12222156, 0.08077106, 0.1501677...   \n",
       "4  [0.1777916, 0.10118558, 0.13280979, 0.10733721...   \n",
       "\n",
       "  user_attrs_val_total_loss   \n",
       "0                  4.715641  \\\n",
       "1                  5.318566   \n",
       "2                  5.665669   \n",
       "3                  2.190145   \n",
       "4                  4.363973   \n",
       "\n",
       "                      user_attrs_val_total_loss_list     state  \n",
       "0  [5.552801609039307, 14.769259452819824, 4.2647...  COMPLETE  \n",
       "1  [8.296555519104004, 6.0792622566223145, 3.8077...  COMPLETE  \n",
       "2  [4.602327346801758, 4.664762020111084, 3.37358...  COMPLETE  \n",
       "3  [1.800790786743164, 1.7654943466186523, 1.7936...  COMPLETE  \n",
       "4  [2.379190444946289, 4.389517784118652, 4.29368...  COMPLETE  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date = \"2023062618\"\n",
    "with open(os.path.join(settings.optimization_path, f'optimization_optuna_{date}.pkl'), 'rb') as file:\n",
    "    optimization_study = dill.load(file)\n",
    "optimization_results = pd.read_csv(os.path.join(settings.optimization_path, f\"optimization_results_{date}.csv\"))\n",
    "best_trial = optimization_results.iloc[0]\n",
    "print(\"optimization_results.shape\", optimization_results.shape)\n",
    "optimization_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6292385-75e1-4041-80a2-c8f256f887c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformed_X.shape (147, 6912)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>AACS</th>\n",
       "      <th>AADAC</th>\n",
       "      <th>AAK1</th>\n",
       "      <th>AAMP</th>\n",
       "      <th>AATK</th>\n",
       "      <th>ABCA1</th>\n",
       "      <th>ABCA3</th>\n",
       "      <th>...</th>\n",
       "      <th>cg27518014</th>\n",
       "      <th>cg27541892</th>\n",
       "      <th>cg27546066</th>\n",
       "      <th>cg27576485</th>\n",
       "      <th>cg27578811</th>\n",
       "      <th>cg27594157</th>\n",
       "      <th>cg27604402</th>\n",
       "      <th>cg27610821</th>\n",
       "      <th>cg27618240</th>\n",
       "      <th>cg27645259</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB6</th>\n",
       "      <td>1.838545</td>\n",
       "      <td>-1.000486</td>\n",
       "      <td>1.095929</td>\n",
       "      <td>-1.405823</td>\n",
       "      <td>-2.010877</td>\n",
       "      <td>0.094786</td>\n",
       "      <td>-1.083251</td>\n",
       "      <td>-0.538234</td>\n",
       "      <td>-0.429991</td>\n",
       "      <td>-1.131272</td>\n",
       "      <td>...</td>\n",
       "      <td>0.104428</td>\n",
       "      <td>1.295505</td>\n",
       "      <td>1.428531</td>\n",
       "      <td>-0.257988</td>\n",
       "      <td>0.883600</td>\n",
       "      <td>1.172822</td>\n",
       "      <td>0.723690</td>\n",
       "      <td>-0.823215</td>\n",
       "      <td>0.992312</td>\n",
       "      <td>2.282258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB8</th>\n",
       "      <td>-0.044025</td>\n",
       "      <td>-0.056539</td>\n",
       "      <td>-0.485821</td>\n",
       "      <td>-0.889678</td>\n",
       "      <td>0.457584</td>\n",
       "      <td>-1.121548</td>\n",
       "      <td>0.487510</td>\n",
       "      <td>0.251208</td>\n",
       "      <td>-0.645695</td>\n",
       "      <td>-1.232429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.479146</td>\n",
       "      <td>-1.141428</td>\n",
       "      <td>-1.643287</td>\n",
       "      <td>-0.938267</td>\n",
       "      <td>0.189676</td>\n",
       "      <td>-0.775050</td>\n",
       "      <td>-0.363433</td>\n",
       "      <td>-0.760774</td>\n",
       "      <td>0.458498</td>\n",
       "      <td>-0.597939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB9</th>\n",
       "      <td>1.415203</td>\n",
       "      <td>0.881504</td>\n",
       "      <td>1.101775</td>\n",
       "      <td>-1.080933</td>\n",
       "      <td>0.098780</td>\n",
       "      <td>0.097744</td>\n",
       "      <td>-0.666669</td>\n",
       "      <td>0.423352</td>\n",
       "      <td>-0.909705</td>\n",
       "      <td>0.408378</td>\n",
       "      <td>...</td>\n",
       "      <td>0.305970</td>\n",
       "      <td>0.003199</td>\n",
       "      <td>-1.253669</td>\n",
       "      <td>-0.097550</td>\n",
       "      <td>-0.451130</td>\n",
       "      <td>-0.048385</td>\n",
       "      <td>0.143848</td>\n",
       "      <td>-0.866286</td>\n",
       "      <td>-0.383553</td>\n",
       "      <td>-0.342804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABA</th>\n",
       "      <td>1.540599</td>\n",
       "      <td>-0.659173</td>\n",
       "      <td>0.233626</td>\n",
       "      <td>0.086201</td>\n",
       "      <td>-0.518542</td>\n",
       "      <td>-0.259241</td>\n",
       "      <td>0.307759</td>\n",
       "      <td>0.576417</td>\n",
       "      <td>0.354872</td>\n",
       "      <td>-1.008569</td>\n",
       "      <td>...</td>\n",
       "      <td>0.123747</td>\n",
       "      <td>-0.694590</td>\n",
       "      <td>0.567955</td>\n",
       "      <td>-0.300843</td>\n",
       "      <td>-1.632633</td>\n",
       "      <td>-0.836188</td>\n",
       "      <td>-0.376244</td>\n",
       "      <td>-1.017153</td>\n",
       "      <td>0.707431</td>\n",
       "      <td>-0.688826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABE</th>\n",
       "      <td>0.542739</td>\n",
       "      <td>-0.529051</td>\n",
       "      <td>0.407191</td>\n",
       "      <td>1.104263</td>\n",
       "      <td>0.301339</td>\n",
       "      <td>-0.609332</td>\n",
       "      <td>0.820578</td>\n",
       "      <td>0.900868</td>\n",
       "      <td>-0.285007</td>\n",
       "      <td>-0.684110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534525</td>\n",
       "      <td>1.630993</td>\n",
       "      <td>1.171886</td>\n",
       "      <td>-1.097332</td>\n",
       "      <td>0.278700</td>\n",
       "      <td>-1.046977</td>\n",
       "      <td>0.008427</td>\n",
       "      <td>1.008485</td>\n",
       "      <td>0.473871</td>\n",
       "      <td>-0.628166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6912 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 A2ML1       A2M    A4GALT      AACS     AADAC      AAK1   \n",
       "TCGA-2J-AAB6  1.838545 -1.000486  1.095929 -1.405823 -2.010877  0.094786  \\\n",
       "TCGA-2J-AAB8 -0.044025 -0.056539 -0.485821 -0.889678  0.457584 -1.121548   \n",
       "TCGA-2J-AAB9  1.415203  0.881504  1.101775 -1.080933  0.098780  0.097744   \n",
       "TCGA-2J-AABA  1.540599 -0.659173  0.233626  0.086201 -0.518542 -0.259241   \n",
       "TCGA-2J-AABE  0.542739 -0.529051  0.407191  1.104263  0.301339 -0.609332   \n",
       "\n",
       "                  AAMP      AATK     ABCA1     ABCA3  ...  cg27518014   \n",
       "TCGA-2J-AAB6 -1.083251 -0.538234 -0.429991 -1.131272  ...    0.104428  \\\n",
       "TCGA-2J-AAB8  0.487510  0.251208 -0.645695 -1.232429  ...    0.479146   \n",
       "TCGA-2J-AAB9 -0.666669  0.423352 -0.909705  0.408378  ...    0.305970   \n",
       "TCGA-2J-AABA  0.307759  0.576417  0.354872 -1.008569  ...    0.123747   \n",
       "TCGA-2J-AABE  0.820578  0.900868 -0.285007 -0.684110  ...    0.534525   \n",
       "\n",
       "              cg27541892  cg27546066  cg27576485  cg27578811  cg27594157   \n",
       "TCGA-2J-AAB6    1.295505    1.428531   -0.257988    0.883600    1.172822  \\\n",
       "TCGA-2J-AAB8   -1.141428   -1.643287   -0.938267    0.189676   -0.775050   \n",
       "TCGA-2J-AAB9    0.003199   -1.253669   -0.097550   -0.451130   -0.048385   \n",
       "TCGA-2J-AABA   -0.694590    0.567955   -0.300843   -1.632633   -0.836188   \n",
       "TCGA-2J-AABE    1.630993    1.171886   -1.097332    0.278700   -1.046977   \n",
       "\n",
       "              cg27604402  cg27610821  cg27618240  cg27645259  \n",
       "TCGA-2J-AAB6    0.723690   -0.823215    0.992312    2.282258  \n",
       "TCGA-2J-AAB8   -0.363433   -0.760774    0.458498   -0.597939  \n",
       "TCGA-2J-AAB9    0.143848   -0.866286   -0.383553   -0.342804  \n",
       "TCGA-2J-AABA   -0.376244   -1.017153    0.707431   -0.688826  \n",
       "TCGA-2J-AABE    0.008427    1.008485    0.473871   -0.628166  \n",
       "\n",
       "[5 rows x 6912 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_Xs = transform_full_dataset(Xs=Xs, fit_pipelines = False, results_folder = settings.results_path)\n",
    "transformed_X = pd.concat(transformed_Xs, axis = 1)\n",
    "print(\"transformed_X.shape\", transformed_X.shape)\n",
    "transformed_X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19465088-e340-4228-9f13-a7c1639a7511",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_study = False\n",
    "if new_study:\n",
    "    date = time.strftime('%Y%m%d%H')\n",
    "    feature_importance_study = optuna.create_study(direction=\"maximize\")\n",
    "    for file in os.listdir(settings.feature_importance_path):\n",
    "        os.remove(os.path.join(settings.feature_importance_path, file))\n",
    "    for view_idx, X in enumerate(transformed_Xs):\n",
    "        features = X.columns.to_list()\n",
    "        for feature_to_drop in features:\n",
    "            feature_importance_study.enqueue_trial({\"view_idx\": view_idx, \"feature_to_drop\": feature_to_drop})\n",
    "    print(\"Features tu explain:\", len(feature_importance_study.trials))\n",
    "\n",
    "else:\n",
    "    date = \"2023062710\"\n",
    "    with open(os.path.join(settings.feature_importance_path, f'feature_importance_results_{date}.pkl'), 'rb') as file:\n",
    "        feature_importance_study = dill.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04148d87-400a-4423-8986-57a6d0fe310f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895712f77b0e498b841aa7e1e1e7b859",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6912 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:35: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  view_idx = trial.system_attrs[\"fixed_params\"][\"view_idx\"]\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:36: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  feature_to_drop = trial.system_attrs[\"fixed_params\"][\"feature_to_drop\"]\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 24.92it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fc2a5450-37ee-4d9f-9715-86211de8c586.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 25.05it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_b90f858b-f539-4636-a796-381ba284b927.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 25.01it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_bfb8758f-7715-4ae2-98d3-c3d495646bbe.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 24.95it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_81d41e3f-9cca-4a3d-a74b-8bc4f47c0ccb.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fc2a5450-37ee-4d9f-9715-86211de8c586.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_b90f858b-f539-4636-a796-381ba284b927.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_bfb8758f-7715-4ae2-98d3-c3d495646bbe.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_81d41e3f-9cca-4a3d-a74b-8bc4f47c0ccb.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  73%|███████▎  | 73/100 [00:00<00:00, 99.48it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 93.96it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6788ca96-f22d-43f0-a3b1-b7f6d95644f4.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6788ca96-f22d-43f0-a3b1-b7f6d95644f4.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   8%|▊         | 8/100 [00:00<00:01, 69.61it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.35it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4462e261-4b61-40c4-b91b-0b9e0930e24d.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.83it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4c5a758f-9cc7-4f23-a2f5-af4125539d3e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4462e261-4b61-40c4-b91b-0b9e0930e24d.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.83it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7091a01a-af07-4be3-807e-9953f0b4ff93.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4c5a758f-9cc7-4f23-a2f5-af4125539d3e.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.39it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d1630234-5ba7-4acd-904b-3e86a2863a1e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7091a01a-af07-4be3-807e-9953f0b4ff93.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d1630234-5ba7-4acd-904b-3e86a2863a1e.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:00<00:00, 101.92it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7dbdd932-dcd3-4c66-8e44-4f6fce16ff72.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7dbdd932-dcd3-4c66-8e44-4f6fce16ff72.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   8%|▊         | 8/100 [00:00<00:01, 70.67it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.95it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7334a0d9-e013-44ff-9c26-59a529868c81.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.30it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8983d86f-93af-4d6e-b531-730262ebcfd5.ckpt\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:03<00:00, 35.31it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7334a0d9-e013-44ff-9c26-59a529868c81.ckpt\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:03<00:00, 30.63it/s]\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_af5b7ac9-03f5-4e4d-9694-1cd34eed8d00.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.52it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e964d63d-5566-447a-9293-9ba4af3879cc.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8983d86f-93af-4d6e-b531-730262ebcfd5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_af5b7ac9-03f5-4e4d-9694-1cd34eed8d00.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e964d63d-5566-447a-9293-9ba4af3879cc.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.354   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:01<00:00, 91.46it/s] \n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8b48142a-44fb-45c2-869d-7b58e70cd129.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8b48142a-44fb-45c2-869d-7b58e70cd129.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 82.20it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 31.49it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_68658ae7-f115-4250-9d07-36e6a7310215.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.55it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_767f5f91-9c5e-4015-b332-b6a7b7b36900.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_68658ae7-f115-4250-9d07-36e6a7310215.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 30.51it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_91982dcd-2c5d-4638-9f68-daddbb50b039.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 39.97it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_767f5f91-9c5e-4015-b332-b6a7b7b36900.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.87it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_ee80f1b1-7766-4a10-a0e4-090c367633d0.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_91982dcd-2c5d-4638-9f68-daddbb50b039.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_ee80f1b1-7766-4a10-a0e4-090c367633d0.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 76.54it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6c4172ef-15a8-4e14-a03d-3fa235f3ed8b.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6c4172ef-15a8-4e14-a03d-3fa235f3ed8b.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:   7%|▋         | 7/100 [00:00<00:01, 68.96it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   6%|▌         | 6/100 [00:00<00:01, 56.11it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.21it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Finding best initial lr:  89%|████████▉ | 89/100 [00:03<00:00, 28.97it/s]Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_479d4ff5-1e5f-4f04-abc8-a8ee1f1c0a38.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.95it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f199794e-81bb-476b-8bc6-1c6d637b221c.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.89it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_00e330e0-52e6-4d7a-9cd0-ec13bfec6fec.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_479d4ff5-1e5f-4f04-abc8-a8ee1f1c0a38.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f199794e-81bb-476b-8bc6-1c6d637b221c.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.59it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_74218bba-3c61-48fc-9965-80bea44f7136.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_00e330e0-52e6-4d7a-9cd0-ec13bfec6fec.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_74218bba-3c61-48fc-9965-80bea44f7136.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  52%|█████▏    | 52/100 [00:00<00:00, 96.62it/s] `Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 90.43it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_87fa27cf-4c9d-4e38-9082-28a92b9d8aac.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_87fa27cf-4c9d-4e38-9082-28a92b9d8aac.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "\u001b[32m[I 2023-06-28 14:32:46,484]\u001b[0m Trial 39 finished with value: 0.02436867356300354 and parameters: {}. Best is trial 15 with value: 0.039337702095508575.\u001b[0m\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:35: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  view_idx = trial.system_attrs[\"fixed_params\"][\"view_idx\"]\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:36: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  feature_to_drop = trial.system_attrs[\"fixed_params\"][\"feature_to_drop\"]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   8%|▊         | 8/100 [00:00<00:01, 77.32it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 31.33it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_82a48ae1-eb59-438a-adf6-6c65ccd62364.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.45it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_024d5ef8-92a9-45a1-9111-5f16fa9603ad.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 30.29it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_9da2fd33-6c5e-40f9-ac92-9abc5fa912d4.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_82a48ae1-eb59-438a-adf6-6c65ccd62364.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.72it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_05745c61-b478-482e-a776-5d79b694dec4.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_024d5ef8-92a9-45a1-9111-5f16fa9603ad.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_9da2fd33-6c5e-40f9-ac92-9abc5fa912d4.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_05745c61-b478-482e-a776-5d79b694dec4.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.354   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 91.17it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f1bc7501-9f32-46c9-8d0d-8ddde36307bc.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f1bc7501-9f32-46c9-8d0d-8ddde36307bc.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 75.63it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.71it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_78363672-3aa7-4a41-8ce0-ec2a7f4c392c.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 29.85it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_c4b58be8-0dbd-49e7-8e78-99e7d3d654d7.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_78363672-3aa7-4a41-8ce0-ec2a7f4c392c.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 35.43it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_c4b58be8-0dbd-49e7-8e78-99e7d3d654d7.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.02it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_b7f4a558-319e-4670-b1be-940b64d99333.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.08it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_44743e7c-3cc0-458b-bce3-709deddcb371.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_b7f4a558-319e-4670-b1be-940b64d99333.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_44743e7c-3cc0-458b-bce3-709deddcb371.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  78%|███████▊  | 78/100 [00:00<00:00, 96.27it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 86.12it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5c466f84-3d89-4e16-b9ea-42a54d7ffddb.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5c466f84-3d89-4e16-b9ea-42a54d7ffddb.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  10%|█         | 10/100 [00:00<00:01, 89.62it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 31.60it/s]\n",
      "Finding best initial lr:  89%|████████▉ | 89/100 [00:02<00:00, 29.06it/s]LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5bde0c76-ab43-4d53-9f50-c0446ebf822f.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.13it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:03<00:00, 33.96it/s]Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d70d1733-3522-45b0-9c97-d0144117a848.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.17it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_9f6a17c7-b9ba-487f-b1a0-586406939980.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5bde0c76-ab43-4d53-9f50-c0446ebf822f.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.68it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6adcaee2-c72f-4c8e-9f8b-3926d1527abb.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_9f6a17c7-b9ba-487f-b1a0-586406939980.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d70d1733-3522-45b0-9c97-d0144117a848.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6adcaee2-c72f-4c8e-9f8b-3926d1527abb.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 88.00it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4ba531a3-ceb9-470f-85c1-73c0d205c3a5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4ba531a3-ceb9-470f-85c1-73c0d205c3a5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 87.58it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  19%|█▉        | 19/100 [00:00<00:00, 90.41it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:02<00:00, 33.37it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a681573c-1743-47e9-9353-2758ef580ecb.ckpt\n",
      "Finding best initial lr:  89%|████████▉ | 89/100 [00:02<00:00, 33.91it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a681573c-1743-47e9-9353-2758ef580ecb.ckpt\n",
      "Finding best initial lr:  93%|█████████▎| 93/100 [00:03<00:00, 35.49it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 36.57it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.72it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4bf56638-ad1f-418c-94a3-b55060e86e5e.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.71it/s]\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.74it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8acf1acd-168d-456d-9697-df3f2a7bb3b4.ckpt\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5ef80dca-861f-40ef-8a4b-4a55d3056581.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4bf56638-ad1f-418c-94a3-b55060e86e5e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8acf1acd-168d-456d-9697-df3f2a7bb3b4.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5ef80dca-861f-40ef-8a4b-4a55d3056581.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.354   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:00<00:00, 101.78it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d7c49cf7-23cf-49f9-8853-3c5072c69332.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d7c49cf7-23cf-49f9-8853-3c5072c69332.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.354   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   6%|▌         | 6/100 [00:00<00:01, 57.27it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.63it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_1fc859ee-bd9b-4e96-b8bc-0c498869ce4c.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.35it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e3d3cd60-888f-4bf1-be72-2909ac46ea50.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.44it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5778ffbe-0ea6-497d-92df-599a0b3411e8.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 29.62it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_882d9c7e-1e08-40af-b1ac-1a7ae4d9386b.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_1fc859ee-bd9b-4e96-b8bc-0c498869ce4c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e3d3cd60-888f-4bf1-be72-2909ac46ea50.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5778ffbe-0ea6-497d-92df-599a0b3411e8.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_882d9c7e-1e08-40af-b1ac-1a7ae4d9386b.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:00<00:00, 99.05it/s] \n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fea254de-8d79-40b1-9a62-b5df452f2e6c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fea254de-8d79-40b1-9a62-b5df452f2e6c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "\u001b[32m[I 2023-06-28 15:05:01,198]\u001b[0m Trial 40 finished with value: 0.02531757950782776 and parameters: {}. Best is trial 15 with value: 0.039337702095508575.\u001b[0m\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:35: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  view_idx = trial.system_attrs[\"fixed_params\"][\"view_idx\"]\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:36: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  feature_to_drop = trial.system_attrs[\"fixed_params\"][\"feature_to_drop\"]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   7%|▋         | 7/100 [00:00<00:01, 64.64it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   6%|▌         | 6/100 [00:00<00:01, 57.48it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.26it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_26fd7e54-cc14-4f18-8b7e-b5fb4b22dd42.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 29.84it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_952627ae-60bc-4e6f-9239-3c4ee610d443.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 29.91it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_1ce445ce-60de-4764-9a54-21728a3f78fd.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_26fd7e54-cc14-4f18-8b7e-b5fb4b22dd42.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_952627ae-60bc-4e6f-9239-3c4ee610d443.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.38it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3eb69600-1132-4e50-af33-c26f176ed3f8.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_1ce445ce-60de-4764-9a54-21728a3f78fd.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3eb69600-1132-4e50-af33-c26f176ed3f8.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.354   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:00<00:00, 99.47it/s] \n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_40eae7d2-4da3-4418-9aca-fcdb7f24b53d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_40eae7d2-4da3-4418-9aca-fcdb7f24b53d.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   6%|▌         | 6/100 [00:00<00:01, 58.35it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.72it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_2d115375-1842-4e81-9e5a-8e02f3b27bca.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.54it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_44254fd1-83e4-4767-8496-96b309c6a383.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 29.73it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8d2681ea-7259-4877-b0ed-b833a01e971d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_2d115375-1842-4e81-9e5a-8e02f3b27bca.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 29.87it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8f0311ca-574a-4d14-9e79-2e8efc89b238.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_44254fd1-83e4-4767-8496-96b309c6a383.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8d2681ea-7259-4877-b0ed-b833a01e971d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8f0311ca-574a-4d14-9e79-2e8efc89b238.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:00<00:00, 100.02it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_89451895-73b7-4d36-a96d-e299d3801163.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_89451895-73b7-4d36-a96d-e299d3801163.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   8%|▊         | 8/100 [00:00<00:01, 78.70it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   7%|▋         | 7/100 [00:00<00:01, 59.69it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.92it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_195f0559-aad9-4035-95a3-95eaf3e885fe.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.24it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3b339f4d-530b-44df-a4f6-57a9c266b107.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_195f0559-aad9-4035-95a3-95eaf3e885fe.ckpt\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:03<00:00, 34.92it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3b339f4d-530b-44df-a4f6-57a9c266b107.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 30.16it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_664e4c06-512f-4669-819b-3f77a701902d.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.24it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e4b80187-07d8-444e-a152-764fb895bd96.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_664e4c06-512f-4669-819b-3f77a701902d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e4b80187-07d8-444e-a152-764fb895bd96.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  11%|█         | 11/100 [00:00<00:01, 50.11it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 83.75it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_aec2a703-08c1-4453-af4d-4808f75ea4cd.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_aec2a703-08c1-4453-af4d-4808f75ea4cd.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   6%|▌         | 6/100 [00:00<00:01, 54.97it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  12%|█▏        | 12/100 [00:00<00:01, 54.25it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:02<00:00, 32.75it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3a4c6f28-a186-4f2f-b340-3744e74d5a39.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 31.93it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6fdb0867-33eb-4c64-8738-20cd2223fb3f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3a4c6f28-a186-4f2f-b340-3744e74d5a39.ckpt\n",
      "Finding best initial lr:  87%|████████▋ | 87/100 [00:02<00:00, 36.95it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6fdb0867-33eb-4c64-8738-20cd2223fb3f.ckpt\n",
      "Finding best initial lr:  93%|█████████▎| 93/100 [00:02<00:00, 42.83it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 31.81it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fa26accd-f49f-4b91-bea5-0582e5ec399e.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.354   Total estimated model params size (MB)\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 45.80it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 32.19it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3ce61678-dd42-4758-b57d-f6afbee35fc6.ckpt\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fa26accd-f49f-4b91-bea5-0582e5ec399e.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3ce61678-dd42-4758-b57d-f6afbee35fc6.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:00<00:00, 101.51it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8aac8760-42a1-4cfe-af5d-e234f0cd8c68.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8aac8760-42a1-4cfe-af5d-e234f0cd8c68.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   7%|▋         | 7/100 [00:00<00:01, 63.63it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.93it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_99d1ecfd-762f-4542-aa51-66c9d548a3b6.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.77it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e0c8f6a8-85e8-40fa-adf0-db8b6f528ca2.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 29.63it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a7685f6a-5b0f-4f19-92d6-7b474b8715b0.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 34.93it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_99d1ecfd-762f-4542-aa51-66c9d548a3b6.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e0c8f6a8-85e8-40fa-adf0-db8b6f528ca2.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.15it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6231c73f-a06a-4d4f-85c3-7b15bb447f24.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a7685f6a-5b0f-4f19-92d6-7b474b8715b0.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6231c73f-a06a-4d4f-85c3-7b15bb447f24.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:00<00:00, 99.11it/s] \n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_2106bc46-56ad-4564-87cb-740187851d69.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_2106bc46-56ad-4564-87cb-740187851d69.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "\u001b[32m[I 2023-06-28 15:33:18,741]\u001b[0m Trial 41 finished with value: 0.012749850749969482 and parameters: {}. Best is trial 15 with value: 0.039337702095508575.\u001b[0m\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:35: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  view_idx = trial.system_attrs[\"fixed_params\"][\"view_idx\"]\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:36: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  feature_to_drop = trial.system_attrs[\"fixed_params\"][\"feature_to_drop\"]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 81.46it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   7%|▋         | 7/100 [00:00<00:01, 57.17it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 31.54it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fb52caa1-2a3a-4eb7-9183-735cd3e52733.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.35it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_36ada095-b274-4056-93e2-54d0a5770d22.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fb52caa1-2a3a-4eb7-9183-735cd3e52733.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 35.73it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_36ada095-b274-4056-93e2-54d0a5770d22.ckpt\n",
      "Finding best initial lr:  96%|█████████▌| 96/100 [00:03<00:00, 37.21it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.29it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_b53c13d6-d61f-4338-9739-2d19c53e0968.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.85it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_36cb9d2f-f19d-4ea4-972a-8bec22222b15.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_b53c13d6-d61f-4338-9739-2d19c53e0968.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_36cb9d2f-f19d-4ea4-972a-8bec22222b15.ckpt\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  11%|█         | 11/100 [00:00<00:00, 100.97it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:00<00:00, 98.05it/s] \n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_c2d8a2e7-198c-4cca-a64f-1d97eed2bdc3.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_c2d8a2e7-198c-4cca-a64f-1d97eed2bdc3.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   7%|▋         | 7/100 [00:00<00:01, 66.20it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  12%|█▏        | 12/100 [00:00<00:01, 52.84it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 31.56it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_433910df-a108-474d-8522-fedf00d27991.ckpt\n",
      "Finding best initial lr:  96%|█████████▌| 96/100 [00:03<00:00, 30.86it/s]\n",
      "LR finder stopped early after 96 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_572889e0-2a4f-46cd-9a3f-00b5610cd20d.ckpt\n",
      "Finding best initial lr:  89%|████████▉ | 89/100 [00:02<00:00, 36.75it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_433910df-a108-474d-8522-fedf00d27991.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_572889e0-2a4f-46cd-9a3f-00b5610cd20d.ckpt\n",
      "Finding best initial lr:  91%|█████████ | 91/100 [00:02<00:00, 37.50it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:03<00:00, 42.04it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 31.36it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e7cbfad8-b351-4007-b2e2-44ab20b56171.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 31.35it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5b2d50af-2960-4fee-b156-85219e90f9bc.ckpt\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e7cbfad8-b351-4007-b2e2-44ab20b56171.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5b2d50af-2960-4fee-b156-85219e90f9bc.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 89.57it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000363078054770101\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d55a343e-a9d5-4f9f-9297-e3a1bee9b3b6.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d55a343e-a9d5-4f9f-9297-e3a1bee9b3b6.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   7%|▋         | 7/100 [00:00<00:01, 62.11it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  11%|█         | 11/100 [00:00<00:02, 41.98it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.44it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_af33063c-a412-427a-964a-67c0cca8bda3.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.05it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6785c490-7f4a-4b66-987e-9a54a8b9b435.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.07it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_c3a65cc8-5395-47ca-b7f9-c7409c2fa33e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_af33063c-a412-427a-964a-67c0cca8bda3.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6785c490-7f4a-4b66-987e-9a54a8b9b435.ckpt\n",
      "Finding best initial lr:  93%|█████████▎| 93/100 [00:03<00:00, 38.36it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_c3a65cc8-5395-47ca-b7f9-c7409c2fa33e.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 31.14it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.000363078054770101\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3c6d57ff-423b-437e-b067-268e044217a8.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3c6d57ff-423b-437e-b067-268e044217a8.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  30%|███       | 30/100 [00:00<00:00, 88.53it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 91.88it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a4a9c78c-bb3c-4499-b556-0f0fbdb03ea5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a4a9c78c-bb3c-4499-b556-0f0fbdb03ea5.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 86.92it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 31.73it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_77fa34ce-3fd8-4133-9259-569397481a52.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 30.12it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0004365158322401656\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f1dee7c6-3063-4328-8968-77cc6f695586.ckpt\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:03<00:00, 33.96it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_77fa34ce-3fd8-4133-9259-569397481a52.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 30.21it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_991c78d4-12a0-49ac-bf2b-016bc425939d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f1dee7c6-3063-4328-8968-77cc6f695586.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.56it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3816a8c0-33fa-40ce-8293-a687ea98f80c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_991c78d4-12a0-49ac-bf2b-016bc425939d.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3816a8c0-33fa-40ce-8293-a687ea98f80c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:01<00:00, 97.54it/s] \n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_b89fd847-7661-4650-8e64-17a0416af71e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_b89fd847-7661-4650-8e64-17a0416af71e.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   8%|▊         | 8/100 [00:00<00:01, 75.02it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:   7%|▋         | 7/100 [00:00<00:01, 61.24it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  14%|█▍        | 14/100 [00:00<00:01, 44.21it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  19%|█▉        | 19/100 [00:00<00:01, 41.25it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 31.79it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_0b9c6c46-f758-43cc-955b-427201156540.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 31.37it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d88cd2a8-faa9-4ff3-8d82-2e2d8e7db765.ckpt\n",
      "Finding best initial lr:  96%|█████████▌| 96/100 [00:03<00:00, 30.95it/s]\n",
      "LR finder stopped early after 96 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_24015a72-911b-4faa-b085-fe344a40c099.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_0b9c6c46-f758-43cc-955b-427201156540.ckpt\n",
      "Finding best initial lr:  84%|████████▍ | 84/100 [00:02<00:00, 37.53it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d88cd2a8-faa9-4ff3-8d82-2e2d8e7db765.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_24015a72-911b-4faa-b085-fe344a40c099.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  93%|█████████▎| 93/100 [00:02<00:00, 52.44it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:02<00:00, 33.89it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_84454177-66c8-4925-9889-66b13cbccec5.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_84454177-66c8-4925-9889-66b13cbccec5.ckpt\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 93.17it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7f6f0a0e-b91c-4745-a4ba-d10ee9efd515.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7f6f0a0e-b91c-4745-a4ba-d10ee9efd515.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "\u001b[32m[I 2023-06-28 16:02:33,633]\u001b[0m Trial 42 finished with value: 0.023414552211761475 and parameters: {}. Best is trial 15 with value: 0.039337702095508575.\u001b[0m\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:35: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  view_idx = trial.system_attrs[\"fixed_params\"][\"view_idx\"]\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:36: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  feature_to_drop = trial.system_attrs[\"fixed_params\"][\"feature_to_drop\"]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 83.20it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]9.53it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  28%|██▊       | 28/100 [00:00<00:01, 71.35it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  36%|███▌      | 36/100 [00:00<00:01, 62.80it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  19%|█▉        | 19/100 [00:00<00:01, 54.94it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:02<00:00, 37.07it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_79f53e12-65b2-42fc-b9fd-0a816e164995.ckpt\n",
      "Finding best initial lr:  87%|████████▋ | 87/100 [00:02<00:00, 33.15it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_79f53e12-65b2-42fc-b9fd-0a816e164995.ckpt\n",
      "Finding best initial lr:  91%|█████████ | 91/100 [00:02<00:00, 34.63it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  76%|███████▌  | 76/100 [00:02<00:00, 35.69it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:02<00:00, 33.69it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a7ab5402-e211-4fed-9363-b0060cebcef9.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.354   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  90%|█████████ | 90/100 [00:02<00:00, 41.32it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a7ab5402-e211-4fed-9363-b0060cebcef9.ckpt\n",
      "Finding best initial lr:  96%|█████████▌| 96/100 [00:02<00:00, 45.76it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:02<00:00, 33.23it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_98d3fcce-2dee-430c-b4cc-1d1a10d1161d.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:02<00:00, 33.62it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_17870ada-816b-47a7-acf2-7d8e0eb8e340.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_98d3fcce-2dee-430c-b4cc-1d1a10d1161d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_17870ada-816b-47a7-acf2-7d8e0eb8e340.ckpt\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.354   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  10%|█         | 10/100 [00:00<00:00, 95.62it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 97.39it/s] \n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e7e252f5-1964-45b7-bd61-f3e86ff95c7f.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e7e252f5-1964-45b7-bd61-f3e86ff95c7f.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 88.20it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  18%|█▊        | 18/100 [00:00<00:01, 71.45it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 32.44it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_0c71f858-0f15-4c00-9483-55bd2e3bbdee.ckpt\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:03<00:00, 33.53it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_0c71f858-0f15-4c00-9483-55bd2e3bbdee.ckpt\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:03<00:00, 34.79it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.46it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d4e74852-f5f4-44bc-ba92-320ead6053f3.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.53it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0010964781961431851\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_78c2ee34-e758-4ccb-aa67-4f1d449ca3bd.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 30.74it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_45532c8e-740f-46c3-abea-290be5644028.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d4e74852-f5f4-44bc-ba92-320ead6053f3.ckpt\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_78c2ee34-e758-4ccb-aa67-4f1d449ca3bd.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_45532c8e-740f-46c3-abea-290be5644028.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  11%|█         | 11/100 [00:00<00:00, 101.99it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  61%|██████    | 61/100 [00:00<00:00, 90.51it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:01<00:00, 89.47it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_1c0a0680-c324-47fa-aa92-77ab0b0305f1.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_1c0a0680-c324-47fa-aa92-77ab0b0305f1.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   8%|▊         | 8/100 [00:00<00:01, 76.19it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.42it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f56f7e7f-8c86-4afa-aa18-35a06087662a.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 29.79it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_82ae2512-41f1-4919-9226-67bac70c215d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f56f7e7f-8c86-4afa-aa18-35a06087662a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_82ae2512-41f1-4919-9226-67bac70c215d.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.85it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_822c5381-ff8b-466c-891f-0397190ee24c.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.18it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_efcbb5ba-d6c2-44d2-a684-d2fc4b62e0e9.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_822c5381-ff8b-466c-891f-0397190ee24c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_efcbb5ba-d6c2-44d2-a684-d2fc4b62e0e9.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:01<00:00, 88.46it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6b1c01e1-5882-45a8-9a6f-de2eb851576c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6b1c01e1-5882-45a8-9a6f-de2eb851576c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 79.86it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 30.95it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_85cc8b7c-71b4-4df3-a814-d97ee96b07a3.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.26it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_0962a33b-5c57-474b-8e50-a2963de12784.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_85cc8b7c-71b4-4df3-a814-d97ee96b07a3.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.03it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5972b79e-c9b8-439d-959d-32821261c264.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.24it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_c1840d11-84d0-4aeb-bfed-04b7d4c5a89d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_0962a33b-5c57-474b-8e50-a2963de12784.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5972b79e-c9b8-439d-959d-32821261c264.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_c1840d11-84d0-4aeb-bfed-04b7d4c5a89d.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:00<00:00, 99.03it/s] \n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_36ce6aaa-03a3-41e2-923d-190860051082.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_36ce6aaa-03a3-41e2-923d-190860051082.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 83.34it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   5%|▌         | 5/100 [00:00<00:02, 47.36it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  10%|█         | 10/100 [00:00<00:02, 42.37it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:02<00:00, 34.16it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e967c496-971c-4829-8b8b-fb5c57bd9cb6.ckpt\n",
      "Finding best initial lr:  92%|█████████▏| 92/100 [00:02<00:00, 34.75it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_e967c496-971c-4829-8b8b-fb5c57bd9cb6.ckpt\n",
      "Finding best initial lr:  84%|████████▍ | 84/100 [00:02<00:00, 35.81it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  88%|████████▊ | 88/100 [00:02<00:00, 36.77it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 31.55it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d498952b-8648-41fb-9deb-01c6b67050fb.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 31.33it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0007585775750291836\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_430ae138-e169-4ee5-b859-01a408116b3e.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  94%|█████████▍| 94/100 [00:02<00:00, 42.83it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_430ae138-e169-4ee5-b859-01a408116b3e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d498952b-8648-41fb-9deb-01c6b67050fb.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:02<00:00, 32.39it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_74492e6e-1b2e-4be9-ac64-5d26ed3385db.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_74492e6e-1b2e-4be9-ac64-5d26ed3385db.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.354   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  31%|███       | 31/100 [00:00<00:00, 97.62it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:01<00:00, 95.09it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_ed9af499-33e7-4492-bc87-6dfa34c0e085.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_ed9af499-33e7-4492-bc87-6dfa34c0e085.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "\u001b[32m[I 2023-06-28 16:31:18,285]\u001b[0m Trial 43 finished with value: 0.014560475945472717 and parameters: {}. Best is trial 15 with value: 0.039337702095508575.\u001b[0m\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:35: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  view_idx = trial.system_attrs[\"fixed_params\"][\"view_idx\"]\n",
      "/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py:36: FutureWarning: system_attrs has been deprecated in v3.1.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.1.0.\n",
      "  feature_to_drop = trial.system_attrs[\"fixed_params\"][\"feature_to_drop\"]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 76.19it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.49it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4472aec1-f262-43fb-827e-6b32081f885d.ckpt\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:03<00:00, 29.51it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7e1655c7-7519-41b8-980d-ecd66a488365.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 29.86it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fa2f081a-22bd-43d1-a679-15fb88560c7d.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 29.87it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000363078054770101\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_1e5f0ec0-62f4-425f-8d25-87af98de09dd.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_4472aec1-f262-43fb-827e-6b32081f885d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7e1655c7-7519-41b8-980d-ecd66a488365.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_1e5f0ec0-62f4-425f-8d25-87af98de09dd.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_fa2f081a-22bd-43d1-a679-15fb88560c7d.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  97%|█████████▋| 97/100 [00:00<00:00, 101.22it/s]\n",
      "LR finder stopped early after 97 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_eb3069dc-cc91-48b4-9a95-842aa6b1558a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_eb3069dc-cc91-48b4-9a95-842aa6b1558a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  19%|█▉        | 19/100 [00:00<00:00, 91.46it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:02<00:00, 33.68it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_ef068259-05da-4e7e-9b9f-ad61da612e8a.ckpt\n",
      "Finding best initial lr:  91%|█████████ | 91/100 [00:02<00:00, 34.46it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_ef068259-05da-4e7e-9b9f-ad61da612e8a.ckpt\n",
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:03<00:00, 35.05it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.59it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_73b940d8-83fa-4e39-b9ce-be8a6e9b6788.ckpt\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.50it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_aa90464a-bb5f-4909-b48b-f07b61e5100e.ckpt\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.81it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d719e7cb-bc28-4dd1-8f3e-9f27a5e9cd06.ckpt\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_73b940d8-83fa-4e39-b9ce-be8a6e9b6788.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_aa90464a-bb5f-4909-b48b-f07b61e5100e.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d719e7cb-bc28-4dd1-8f3e-9f27a5e9cd06.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 87.67it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  73%|███████▎  | 73/100 [00:00<00:00, 84.38it/s]`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 82.90it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8a7d7424-52aa-4507-a0d3-b4f706a32e63.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_8a7d7424-52aa-4507-a0d3-b4f706a32e63.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:   6%|▌         | 6/100 [00:00<00:01, 57.17it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:03<00:00, 30.73it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_222eb6e0-0b14-466c-9745-b2788dea8e9d.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.16it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_cf919baf-6732-4a3b-9704-52776374cfb0.ckpt\n",
      "Finding best initial lr:  90%|█████████ | 90/100 [00:03<00:00, 31.11it/s]Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_222eb6e0-0b14-466c-9745-b2788dea8e9d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_cf919baf-6732-4a3b-9704-52776374cfb0.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.18it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3736b24a-c8d3-478c-af09-932aef6e8df5.ckpt\n",
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:03<00:00, 30.64it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_09fe7070-a572-452b-a1a0-3e96aa647203.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_3736b24a-c8d3-478c-af09-932aef6e8df5.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_09fe7070-a572-452b-a1a0-3e96aa647203.ckpt\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr: 100%|██████████| 100/100 [00:01<00:00, 87.35it/s]\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_86287af3-de30-454d-a83d-ec3ea91622e1.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_86287af3-de30-454d-a83d-ec3ea91622e1.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:612: UserWarning: Checkpoint directory /home/alberto/Work/course_interpretability_deep_learning/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 10.7 M\n",
      "1 | decoder_0 | FCN  | 10.9 M\n",
      "2 | encoder_1 | FCN  | 2.7 M \n",
      "3 | decoder_1 | FCN  | 2.8 M \n",
      "-----------------------------------\n",
      "27.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "27.1 M    Total params\n",
      "108.335   Total estimated model params size (MB)\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "`Trainer.fit` stopped: `max_epochs=80` reached.\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n",
      "\u001b[33m[W 2023-06-28 16:48:11,790]\u001b[0m Trial 44 failed with parameters: {} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/parallel.py\", line 975, in retrieve\n",
      "    self._output.extend(job.get(timeout=self.timeout))\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 567, in wrap_future_result\n",
      "    return future.result(timeout=timeout)\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/concurrent/futures/_base.py\", line 453, in result\n",
      "    self._condition.wait(timeout)\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/threading.py\", line 320, in wait\n",
      "    waiter.acquire()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/parallel.py\", line 1098, in __call__\n",
      "    self.retrieve()\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/parallel.py\", line 997, in retrieve\n",
      "    backend.abort_everything(ensure_ready=ensure_ready)\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 586, in abort_everything\n",
      "    self._workers.terminate(kill_workers=True)\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/executor.py\", line 74, in terminate\n",
      "    self.shutdown(kill_workers=kill_workers)\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/externals/loky/process_executor.py\", line 1199, in shutdown\n",
      "    executor_manager_thread.join()\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/threading.py\", line 1096, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
      "    if lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_297786/685350380.py\", line 1, in <lambda>\n",
      "    func_objective = lambda trial: FeatureImportance().objective(trial= trial, Xs= Xs, samples= samples, original_score = optimization_study.best_value,\n",
      "  File \"/home/alberto/Work/course_interpretability_deep_learning/src/explainability/feature_importance_analysis.py\", line 44, in objective\n",
      "    results_step = Parallel(n_jobs=n_jobs)(delayed(self._step)(Xs_provtrain=Xs_provtrain, Xs_provtest=Xs_provtest,\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/parallel.py\", line 1108, in __call__\n",
      "    self._terminate_backend()\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/parallel.py\", line 799, in _terminate_backend\n",
      "    self._backend.terminate()\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/_parallel_backends.py\", line 576, in terminate\n",
      "    self._workers._temp_folder_manager._unlink_temporary_resources(\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/_memmapping_reducer.py\", line 631, in _unlink_temporary_resources\n",
      "    self._try_delete_folder(\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/_memmapping_reducer.py\", line 658, in _try_delete_folder\n",
      "    delete_folder(\n",
      "  File \"/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/joblib/disk.py\", line 136, in delete_folder\n",
      "    time.sleep(RM_SUBDIRS_RETRY_TIME)\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-06-28 16:48:11,795]\u001b[0m Trial 44 failed with value None.\u001b[0m\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "func_objective = lambda trial: FeatureImportance().objective(trial= trial, Xs= Xs, samples= samples, original_score = optimization_study.best_value,\n",
    "                                                             features_per_component = optimization_study.best_params[\"features_per_component\"],\n",
    "                                                             in_channels_list = eval(best_trial[\"user_attrs_num_features\"]),\n",
    "                                                             hidden_channels_list = [view_hidden[1:] for view_hidden in eval(best_trial[\"user_attrs_num_units\"])],\n",
    "                                                             n_clusters = optimization_study.best_params[\"n_clusters\"],\n",
    "                                                             n_epochs = optimization_study.best_params[\"n_epochs\"],\n",
    "                                                             lambda_coeff = optimization_study.best_params[\"lambda_coeff\"],\n",
    "                                                             optimization_folder = settings.optimization_path,\n",
    "                                                             random_state=settings.RANDOM_STATE, n_jobs= 4, folder= settings.feature_importance_path)\n",
    "\n",
    "keep_trying = True\n",
    "while keep_trying:\n",
    "    try:\n",
    "        feature_importance_study = FeatureImportance.optimize_optuna_and_save(study= feature_importance_study, n_trials = len(feature_importance_study.trials), date=date, \n",
    "                                                                              show_progress_bar= True, folder= settings.feature_importance_path, func= func_objective)\n",
    "        if new_study:\n",
    "            keep_trying = False\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "    except ValueError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e8984957-a31d-4c13-aa25-70bad5d25db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='RNA-seq'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "current_study = feature_importance_study.trials_dataframe()[[\"system_attrs_fixed_params\", \"value\"]].dropna().sort_values(\"value\", ascending = False)\n",
    "current_study[\"system_attrs_fixed_params\"] = current_study[\"system_attrs_fixed_params\"].apply(lambda x: x[\"feature_to_drop\"])\n",
    "current_study.plot.bar(x='system_attrs_fixed_params', y='value', xlabel = \"RNA-seq\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
