{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4615a83d-b9cd-4c1d-84df-d5e75c894c71",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:01.387894429Z",
     "start_time": "2023-05-23T15:01:01.366280485Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/alberto/Work/course_interpretability_deep_learning\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a073f0-ba9d-4545-a0f4-a49d638b7695",
   "metadata": {},
   "source": [
    "# Multi-omics stratification on PDAC patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa7dffc4-d560-47bb-84b8-a6870d4824a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:03.420757620Z",
     "start_time": "2023-05-23T15:01:03.416255778Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "import optuna\n",
    "import time\n",
    "import dill\n",
    "import shutil\n",
    "\n",
    "from src import settings\n",
    "from utils import RemoveFeaturesWithZeros, RemoveFeaturesWithNaN, FeatureSelectionNMF, RemoveCorrelatedFeatures, RemoveFeaturesLowMAE\n",
    "from optimization import Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5991e212-7498-4da5-b089-c355594b1113",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525e21f1-eb98-424d-91e1-328329b4e786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:09.194541257Z",
     "start_time": "2023-05-23T15:01:05.607110Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "methylation_data.shape (153, 301195)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cg00000029</th>\n",
       "      <th>cg00000236</th>\n",
       "      <th>cg00000289</th>\n",
       "      <th>cg00000292</th>\n",
       "      <th>cg00000321</th>\n",
       "      <th>cg00000622</th>\n",
       "      <th>cg00000658</th>\n",
       "      <th>cg00000714</th>\n",
       "      <th>cg00000721</th>\n",
       "      <th>cg00000734</th>\n",
       "      <th>...</th>\n",
       "      <th>ch.9.2262725R</th>\n",
       "      <th>ch.9.2285199R</th>\n",
       "      <th>ch.9.2298007R</th>\n",
       "      <th>ch.9.2473665R</th>\n",
       "      <th>ch.9.357218F</th>\n",
       "      <th>ch.9.377428R</th>\n",
       "      <th>ch.9.691424R</th>\n",
       "      <th>ch.9.837340R</th>\n",
       "      <th>ch.9.898515R</th>\n",
       "      <th>ch.9.991104F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB6</th>\n",
       "      <td>0.157951</td>\n",
       "      <td>0.836226</td>\n",
       "      <td>0.710511</td>\n",
       "      <td>0.560780</td>\n",
       "      <td>0.239194</td>\n",
       "      <td>0.016433</td>\n",
       "      <td>0.864604</td>\n",
       "      <td>0.087681</td>\n",
       "      <td>0.938775</td>\n",
       "      <td>0.061008</td>\n",
       "      <td>...</td>\n",
       "      <td>0.103136</td>\n",
       "      <td>0.053757</td>\n",
       "      <td>0.032478</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.064965</td>\n",
       "      <td>0.049776</td>\n",
       "      <td>0.115268</td>\n",
       "      <td>0.095954</td>\n",
       "      <td>0.084203</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB8</th>\n",
       "      <td>0.300754</td>\n",
       "      <td>0.782242</td>\n",
       "      <td>0.574296</td>\n",
       "      <td>0.670286</td>\n",
       "      <td>0.424310</td>\n",
       "      <td>0.014747</td>\n",
       "      <td>0.885958</td>\n",
       "      <td>0.112524</td>\n",
       "      <td>0.930765</td>\n",
       "      <td>0.037198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.028180</td>\n",
       "      <td>0.054483</td>\n",
       "      <td>0.022736</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.060835</td>\n",
       "      <td>0.036434</td>\n",
       "      <td>0.160082</td>\n",
       "      <td>0.059216</td>\n",
       "      <td>0.065342</td>\n",
       "      <td>0.166304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB9</th>\n",
       "      <td>0.257807</td>\n",
       "      <td>0.846522</td>\n",
       "      <td>0.534748</td>\n",
       "      <td>0.688073</td>\n",
       "      <td>0.295597</td>\n",
       "      <td>0.014649</td>\n",
       "      <td>0.895039</td>\n",
       "      <td>0.167297</td>\n",
       "      <td>0.940112</td>\n",
       "      <td>0.058407</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059313</td>\n",
       "      <td>0.063187</td>\n",
       "      <td>0.032581</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.055342</td>\n",
       "      <td>0.069086</td>\n",
       "      <td>0.128546</td>\n",
       "      <td>0.120015</td>\n",
       "      <td>0.074940</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABA</th>\n",
       "      <td>0.239086</td>\n",
       "      <td>0.789457</td>\n",
       "      <td>0.474723</td>\n",
       "      <td>0.705372</td>\n",
       "      <td>0.530321</td>\n",
       "      <td>0.016919</td>\n",
       "      <td>0.884874</td>\n",
       "      <td>0.129581</td>\n",
       "      <td>0.910885</td>\n",
       "      <td>0.062167</td>\n",
       "      <td>...</td>\n",
       "      <td>0.122677</td>\n",
       "      <td>0.056068</td>\n",
       "      <td>0.023190</td>\n",
       "      <td>0.109351</td>\n",
       "      <td>0.056015</td>\n",
       "      <td>0.053238</td>\n",
       "      <td>0.082979</td>\n",
       "      <td>0.057172</td>\n",
       "      <td>0.045781</td>\n",
       "      <td>0.121676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABE</th>\n",
       "      <td>0.168622</td>\n",
       "      <td>0.841684</td>\n",
       "      <td>0.591205</td>\n",
       "      <td>0.623799</td>\n",
       "      <td>0.322576</td>\n",
       "      <td>0.014408</td>\n",
       "      <td>0.898202</td>\n",
       "      <td>0.125415</td>\n",
       "      <td>0.941153</td>\n",
       "      <td>0.059365</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046699</td>\n",
       "      <td>0.049177</td>\n",
       "      <td>0.032707</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.075854</td>\n",
       "      <td>0.062602</td>\n",
       "      <td>0.122072</td>\n",
       "      <td>0.082753</td>\n",
       "      <td>0.071240</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 301195 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              cg00000029  cg00000236  cg00000289  cg00000292  cg00000321   \n",
       "TCGA-2J-AAB6    0.157951    0.836226    0.710511    0.560780    0.239194  \\\n",
       "TCGA-2J-AAB8    0.300754    0.782242    0.574296    0.670286    0.424310   \n",
       "TCGA-2J-AAB9    0.257807    0.846522    0.534748    0.688073    0.295597   \n",
       "TCGA-2J-AABA    0.239086    0.789457    0.474723    0.705372    0.530321   \n",
       "TCGA-2J-AABE    0.168622    0.841684    0.591205    0.623799    0.322576   \n",
       "\n",
       "              cg00000622  cg00000658  cg00000714  cg00000721  cg00000734  ...   \n",
       "TCGA-2J-AAB6    0.016433    0.864604    0.087681    0.938775    0.061008  ...  \\\n",
       "TCGA-2J-AAB8    0.014747    0.885958    0.112524    0.930765    0.037198  ...   \n",
       "TCGA-2J-AAB9    0.014649    0.895039    0.167297    0.940112    0.058407  ...   \n",
       "TCGA-2J-AABA    0.016919    0.884874    0.129581    0.910885    0.062167  ...   \n",
       "TCGA-2J-AABE    0.014408    0.898202    0.125415    0.941153    0.059365  ...   \n",
       "\n",
       "              ch.9.2262725R  ch.9.2285199R  ch.9.2298007R  ch.9.2473665R   \n",
       "TCGA-2J-AAB6       0.103136       0.053757       0.032478            NaN  \\\n",
       "TCGA-2J-AAB8       0.028180       0.054483       0.022736            NaN   \n",
       "TCGA-2J-AAB9       0.059313       0.063187       0.032581            NaN   \n",
       "TCGA-2J-AABA       0.122677       0.056068       0.023190       0.109351   \n",
       "TCGA-2J-AABE       0.046699       0.049177       0.032707            NaN   \n",
       "\n",
       "              ch.9.357218F  ch.9.377428R  ch.9.691424R  ch.9.837340R   \n",
       "TCGA-2J-AAB6      0.064965      0.049776      0.115268      0.095954  \\\n",
       "TCGA-2J-AAB8      0.060835      0.036434      0.160082      0.059216   \n",
       "TCGA-2J-AAB9      0.055342      0.069086      0.128546      0.120015   \n",
       "TCGA-2J-AABA      0.056015      0.053238      0.082979      0.057172   \n",
       "TCGA-2J-AABE      0.075854      0.062602      0.122072      0.082753   \n",
       "\n",
       "              ch.9.898515R  ch.9.991104F  \n",
       "TCGA-2J-AAB6      0.084203           NaN  \n",
       "TCGA-2J-AAB8      0.065342      0.166304  \n",
       "TCGA-2J-AAB9      0.074940           NaN  \n",
       "TCGA-2J-AABA      0.045781      0.121676  \n",
       "TCGA-2J-AABE      0.071240           NaN  \n",
       "\n",
       "[5 rows x 301195 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methylation_data = pd.read_csv(settings.methylation_data_path, sep=\";\", index_col=0, decimal=\",\")\n",
    "methylation_data.columns = methylation_data.columns.str.replace(\".\", \"-\")\n",
    "methylation_data = methylation_data.T\n",
    "methylation_data = methylation_data.astype(np.float32)\n",
    "print(\"methylation_data.shape\", methylation_data.shape)\n",
    "methylation_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ba9fa3a-ba17-49a1-86ef-6b399dfaafed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:09.359360346Z",
     "start_time": "2023-05-23T15:01:09.194876421Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnaseq_data.shape (147, 20501)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1BG</th>\n",
       "      <th>A1CF</th>\n",
       "      <th>A2BP1</th>\n",
       "      <th>A2LD1</th>\n",
       "      <th>A2ML1</th>\n",
       "      <th>A2M</th>\n",
       "      <th>A4GALT</th>\n",
       "      <th>A4GNT</th>\n",
       "      <th>AAA1</th>\n",
       "      <th>AAAS</th>\n",
       "      <th>...</th>\n",
       "      <th>ZXDA</th>\n",
       "      <th>ZXDB</th>\n",
       "      <th>ZXDC</th>\n",
       "      <th>ZYG11A</th>\n",
       "      <th>ZYG11B</th>\n",
       "      <th>ZYX</th>\n",
       "      <th>ZZEF1</th>\n",
       "      <th>ZZZ3</th>\n",
       "      <th>psiTPTE22</th>\n",
       "      <th>tAKR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB6</th>\n",
       "      <td>82.549698</td>\n",
       "      <td>8.187100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>163.122803</td>\n",
       "      <td>1815.789551</td>\n",
       "      <td>8517.444336</td>\n",
       "      <td>1121.052612</td>\n",
       "      <td>1.169600</td>\n",
       "      <td>1.1696</td>\n",
       "      <td>834.502930</td>\n",
       "      <td>...</td>\n",
       "      <td>14.619900</td>\n",
       "      <td>269.005798</td>\n",
       "      <td>1053.216431</td>\n",
       "      <td>0.5848</td>\n",
       "      <td>683.625671</td>\n",
       "      <td>11696.491211</td>\n",
       "      <td>869.005798</td>\n",
       "      <td>601.754395</td>\n",
       "      <td>26.315800</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB8</th>\n",
       "      <td>56.930698</td>\n",
       "      <td>33.842499</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>185.814301</td>\n",
       "      <td>16.921301</td>\n",
       "      <td>14413.913086</td>\n",
       "      <td>392.949493</td>\n",
       "      <td>9.400700</td>\n",
       "      <td>0.9401</td>\n",
       "      <td>801.880127</td>\n",
       "      <td>...</td>\n",
       "      <td>35.722698</td>\n",
       "      <td>356.286713</td>\n",
       "      <td>829.142212</td>\n",
       "      <td>3.7603</td>\n",
       "      <td>680.611023</td>\n",
       "      <td>5829.377441</td>\n",
       "      <td>828.202087</td>\n",
       "      <td>609.165710</td>\n",
       "      <td>85.546402</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AAB9</th>\n",
       "      <td>105.787804</td>\n",
       "      <td>21.436199</td>\n",
       "      <td>1.0718</td>\n",
       "      <td>166.709503</td>\n",
       "      <td>642.015015</td>\n",
       "      <td>24311.779297</td>\n",
       "      <td>1125.401855</td>\n",
       "      <td>50.375099</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>862.808105</td>\n",
       "      <td>...</td>\n",
       "      <td>57.877800</td>\n",
       "      <td>381.564789</td>\n",
       "      <td>936.763123</td>\n",
       "      <td>1.0718</td>\n",
       "      <td>646.302307</td>\n",
       "      <td>8094.319336</td>\n",
       "      <td>1083.601318</td>\n",
       "      <td>573.419128</td>\n",
       "      <td>30.010700</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABA</th>\n",
       "      <td>99.345497</td>\n",
       "      <td>18.788200</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>99.276703</td>\n",
       "      <td>873.649597</td>\n",
       "      <td>10302.006836</td>\n",
       "      <td>633.161072</td>\n",
       "      <td>6.262700</td>\n",
       "      <td>18.7882</td>\n",
       "      <td>623.767029</td>\n",
       "      <td>...</td>\n",
       "      <td>52.606899</td>\n",
       "      <td>293.721588</td>\n",
       "      <td>1511.820923</td>\n",
       "      <td>1.2525</td>\n",
       "      <td>945.670898</td>\n",
       "      <td>4829.810547</td>\n",
       "      <td>1364.646851</td>\n",
       "      <td>793.486816</td>\n",
       "      <td>31.313601</td>\n",
       "      <td>0.6263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TCGA-2J-AABE</th>\n",
       "      <td>79.401901</td>\n",
       "      <td>3.083100</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>134.564499</td>\n",
       "      <td>74.610802</td>\n",
       "      <td>11076.861328</td>\n",
       "      <td>710.343811</td>\n",
       "      <td>35.147202</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>702.327698</td>\n",
       "      <td>...</td>\n",
       "      <td>56.728802</td>\n",
       "      <td>431.632507</td>\n",
       "      <td>1069.215454</td>\n",
       "      <td>0.6166</td>\n",
       "      <td>564.205322</td>\n",
       "      <td>7464.775879</td>\n",
       "      <td>832.434082</td>\n",
       "      <td>468.629608</td>\n",
       "      <td>48.096199</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 20501 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    A1BG       A1CF   A2BP1       A2LD1        A2ML1   \n",
       "TCGA-2J-AAB6   82.549698   8.187100  0.0000  163.122803  1815.789551  \\\n",
       "TCGA-2J-AAB8   56.930698  33.842499  0.0000  185.814301    16.921301   \n",
       "TCGA-2J-AAB9  105.787804  21.436199  1.0718  166.709503   642.015015   \n",
       "TCGA-2J-AABA   99.345497  18.788200  0.0000   99.276703   873.649597   \n",
       "TCGA-2J-AABE   79.401901   3.083100  0.0000  134.564499    74.610802   \n",
       "\n",
       "                       A2M       A4GALT      A4GNT     AAA1        AAAS  ...   \n",
       "TCGA-2J-AAB6   8517.444336  1121.052612   1.169600   1.1696  834.502930  ...  \\\n",
       "TCGA-2J-AAB8  14413.913086   392.949493   9.400700   0.9401  801.880127  ...   \n",
       "TCGA-2J-AAB9  24311.779297  1125.401855  50.375099   0.0000  862.808105  ...   \n",
       "TCGA-2J-AABA  10302.006836   633.161072   6.262700  18.7882  623.767029  ...   \n",
       "TCGA-2J-AABE  11076.861328   710.343811  35.147202   0.0000  702.327698  ...   \n",
       "\n",
       "                   ZXDA        ZXDB         ZXDC  ZYG11A      ZYG11B   \n",
       "TCGA-2J-AAB6  14.619900  269.005798  1053.216431  0.5848  683.625671  \\\n",
       "TCGA-2J-AAB8  35.722698  356.286713   829.142212  3.7603  680.611023   \n",
       "TCGA-2J-AAB9  57.877800  381.564789   936.763123  1.0718  646.302307   \n",
       "TCGA-2J-AABA  52.606899  293.721588  1511.820923  1.2525  945.670898   \n",
       "TCGA-2J-AABE  56.728802  431.632507  1069.215454  0.6166  564.205322   \n",
       "\n",
       "                       ZYX        ZZEF1        ZZZ3  psiTPTE22    tAKR  \n",
       "TCGA-2J-AAB6  11696.491211   869.005798  601.754395  26.315800  0.0000  \n",
       "TCGA-2J-AAB8   5829.377441   828.202087  609.165710  85.546402  0.0000  \n",
       "TCGA-2J-AAB9   8094.319336  1083.601318  573.419128  30.010700  0.0000  \n",
       "TCGA-2J-AABA   4829.810547  1364.646851  793.486816  31.313601  0.6263  \n",
       "TCGA-2J-AABE   7464.775879   832.434082  468.629608  48.096199  0.0000  \n",
       "\n",
       "[5 rows x 20501 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnaseq_data = pd.read_csv(settings.rnaseq_data_path, sep=\";\", index_col=0, decimal=\",\")\n",
    "rnaseq_data = rnaseq_data.T\n",
    "rnaseq_data = rnaseq_data.astype(np.float32)\n",
    "print(\"rnaseq_data.shape\", rnaseq_data.shape)\n",
    "rnaseq_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e9f89678-93ce-419e-9d58-386a8d45026f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:11.293572678Z",
     "start_time": "2023-05-23T15:01:11.249405517Z"
    }
   },
   "outputs": [],
   "source": [
    "samples = methylation_data.index.intersection(rnaseq_data.index)\n",
    "methylation_data = methylation_data.loc[samples]\n",
    "rnaseq_data = rnaseq_data.loc[samples]\n",
    "assert methylation_data.index.equals(rnaseq_data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7eb6a7dd-df0c-4de5-92b1-c9846585413a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:11.717965324Z",
     "start_time": "2023-05-23T15:01:11.697010222Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;removefeatureswithzeros&#x27;,\n",
       "                 RemoveFeaturesWithZeros(verbose=True)),\n",
       "                (&#x27;removefeatureslowmae&#x27;,\n",
       "                 RemoveFeaturesLowMAE(percentage_to_keep=0.5, verbose=True)),\n",
       "                (&#x27;removecorrelatedfeatures&#x27;,\n",
       "                 RemoveCorrelatedFeatures(threshold=0.85, verbose=True)),\n",
       "                (&#x27;functiontransformer&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x7efc14b92950&gt;)),\n",
       "                (&#x27;featureselectionnmf&#x27;,\n",
       "                 FeatureSelectionNMF(nmf=NMF(max_iter=10000, n_components=100,\n",
       "                                             random_state=42),\n",
       "                                     verbose=True)),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;removefeatureswithzeros&#x27;,\n",
       "                 RemoveFeaturesWithZeros(verbose=True)),\n",
       "                (&#x27;removefeatureslowmae&#x27;,\n",
       "                 RemoveFeaturesLowMAE(percentage_to_keep=0.5, verbose=True)),\n",
       "                (&#x27;removecorrelatedfeatures&#x27;,\n",
       "                 RemoveCorrelatedFeatures(threshold=0.85, verbose=True)),\n",
       "                (&#x27;functiontransformer&#x27;,\n",
       "                 FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x7efc14b92950&gt;)),\n",
       "                (&#x27;featureselectionnmf&#x27;,\n",
       "                 FeatureSelectionNMF(nmf=NMF(max_iter=10000, n_components=100,\n",
       "                                             random_state=42),\n",
       "                                     verbose=True)),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemoveFeaturesWithZeros</label><div class=\"sk-toggleable__content\"><pre>RemoveFeaturesWithZeros(verbose=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemoveFeaturesLowMAE</label><div class=\"sk-toggleable__content\"><pre>RemoveFeaturesLowMAE(percentage_to_keep=0.5, verbose=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemoveCorrelatedFeatures</label><div class=\"sk-toggleable__content\"><pre>RemoveCorrelatedFeatures(threshold=0.85, verbose=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">FunctionTransformer</label><div class=\"sk-toggleable__content\"><pre>FunctionTransformer(func=&lt;function &lt;lambda&gt; at 0x7efc14b92950&gt;)</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">featureselectionnmf: FeatureSelectionNMF</label><div class=\"sk-toggleable__content\"><pre>FeatureSelectionNMF(nmf=NMF(max_iter=10000, n_components=100, random_state=42),\n",
       "                    verbose=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">nmf: NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(max_iter=10000, n_components=100, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(max_iter=10000, n_components=100, random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('removefeatureswithzeros',\n",
       "                 RemoveFeaturesWithZeros(verbose=True)),\n",
       "                ('removefeatureslowmae',\n",
       "                 RemoveFeaturesLowMAE(percentage_to_keep=0.5, verbose=True)),\n",
       "                ('removecorrelatedfeatures',\n",
       "                 RemoveCorrelatedFeatures(threshold=0.85, verbose=True)),\n",
       "                ('functiontransformer',\n",
       "                 FunctionTransformer(func=<function <lambda> at 0x7efc14b92950>)),\n",
       "                ('featureselectionnmf',\n",
       "                 FeatureSelectionNMF(nmf=NMF(max_iter=10000, n_components=100,\n",
       "                                             random_state=42),\n",
       "                                     verbose=True)),\n",
       "                ('standardscaler', StandardScaler())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnaseq_pipeline = make_pipeline(\n",
    "    RemoveFeaturesWithZeros(threshold= 0.2, verbose= True),\n",
    "    RemoveFeaturesLowMAE(percentage_to_keep= 0.5, verbose= True),\n",
    "    RemoveCorrelatedFeatures(threshold = 0.85, verbose= True),\n",
    "    FunctionTransformer(lambda x: np.log2(1 + x)),\n",
    "    FeatureSelectionNMF(nmf = NMF(n_components= 100, max_iter=10000, random_state=settings.RANDOM_STATE), verbose= True),\n",
    "    StandardScaler().set_output(transform= 'pandas'),\n",
    ")\n",
    "rnaseq_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d6bad2f-d1d8-492b-a50c-e3ddee6080dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:13.085530690Z",
     "start_time": "2023-05-23T15:01:13.076419395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;removefeatureswithnan&#x27;, RemoveFeaturesWithNaN(verbose=True)),\n",
       "                (&#x27;removefeatureslowmae&#x27;,\n",
       "                 RemoveFeaturesLowMAE(percentage_to_keep=0.1, verbose=True)),\n",
       "                (&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;featureselectionnmf&#x27;,\n",
       "                 FeatureSelectionNMF(nmf=NMF(max_iter=10000, n_components=100,\n",
       "                                             random_state=42),\n",
       "                                     verbose=True)),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;removefeatureswithnan&#x27;, RemoveFeaturesWithNaN(verbose=True)),\n",
       "                (&#x27;removefeatureslowmae&#x27;,\n",
       "                 RemoveFeaturesLowMAE(percentage_to_keep=0.1, verbose=True)),\n",
       "                (&#x27;simpleimputer&#x27;, SimpleImputer()),\n",
       "                (&#x27;featureselectionnmf&#x27;,\n",
       "                 FeatureSelectionNMF(nmf=NMF(max_iter=10000, n_components=100,\n",
       "                                             random_state=42),\n",
       "                                     verbose=True)),\n",
       "                (&#x27;standardscaler&#x27;, StandardScaler())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemoveFeaturesWithNaN</label><div class=\"sk-toggleable__content\"><pre>RemoveFeaturesWithNaN(verbose=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RemoveFeaturesLowMAE</label><div class=\"sk-toggleable__content\"><pre>RemoveFeaturesLowMAE(percentage_to_keep=0.1, verbose=True)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">featureselectionnmf: FeatureSelectionNMF</label><div class=\"sk-toggleable__content\"><pre>FeatureSelectionNMF(nmf=NMF(max_iter=10000, n_components=100, random_state=42),\n",
       "                    verbose=True)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">nmf: NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(max_iter=10000, n_components=100, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(max_iter=10000, n_components=100, random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('removefeatureswithnan', RemoveFeaturesWithNaN(verbose=True)),\n",
       "                ('removefeatureslowmae',\n",
       "                 RemoveFeaturesLowMAE(percentage_to_keep=0.1, verbose=True)),\n",
       "                ('simpleimputer', SimpleImputer()),\n",
       "                ('featureselectionnmf',\n",
       "                 FeatureSelectionNMF(nmf=NMF(max_iter=10000, n_components=100,\n",
       "                                             random_state=42),\n",
       "                                     verbose=True)),\n",
       "                ('standardscaler', StandardScaler())])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methylation_pipeline = make_pipeline(\n",
    "    RemoveFeaturesWithNaN(threshold = 0.2, verbose= True),\n",
    "    RemoveFeaturesLowMAE(percentage_to_keep= 0.1, verbose= True),\n",
    "    # RemoveCorrelatedFeatures(threshold = 0.85, verbose= True),\n",
    "    SimpleImputer(strategy= \"mean\").set_output(transform= 'pandas'),\n",
    "    FeatureSelectionNMF(nmf = NMF(n_components= 100, max_iter=10000, random_state=settings.RANDOM_STATE), verbose= True),\n",
    "    StandardScaler().set_output(transform= 'pandas'),\n",
    ")\n",
    "methylation_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3365e0c-aae0-4445-818f-ef2c484769bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:14.675305002Z",
     "start_time": "2023-05-23T15:01:14.622202615Z"
    }
   },
   "outputs": [],
   "source": [
    "# shutil.rmtree(\"lightning_logs/\", ignore_errors= True)\n",
    "# shutil.rmtree(\"checkpoints/\", ignore_errors= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b13f9a31-7086-482a-88b6-1d7d34708c43",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-23T15:01:44.123341892Z",
     "start_time": "2023-05-23T15:01:28.472433387Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-27 10:56:46,182]\u001b[0m A new study created in memory with name: no-name-5f02b886-1905-454f-9434-9149d0995f66\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "func_objective = lambda trial: Optimization.objective(trial= trial, Xs= [rnaseq_data, methylation_data], samples= samples,\n",
    "                                                      pipelines= [rnaseq_pipeline, methylation_pipeline], \n",
    "                                                      max_features= 5000, num_layers_option= [1,2], num_units_option= [2,10],\n",
    "                                                      n_clusters_option= [2, 6], random_state=settings.RANDOM_STATE, n_jobs= 4)\n",
    "\n",
    "new_study = True\n",
    "if new_study:\n",
    "    shutil.rmtree(\"tensorboard/\", ignore_errors= True)\n",
    "    date = time.strftime('%Y%m%d%H')\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    for file in os.listdir(settings.RESULTS_FOLDER):\n",
    "        os.remove(os.path.join(settings.RESULTS_FOLDER, file))\n",
    "else:\n",
    "    date = \"\"\n",
    "    with open(os.path.join(settings.RESULTS_FOLDER, f'optimization_optuna_{date}.pkl'), 'rb') as file:\n",
    "        study = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1016923-cb4c-4a4f-a4b5-398eb4ed0aec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbd60633a08d4c4991d8ea31eb5b1930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/optuna/distributions.py:716: UserWarning: The distribution is specified by [310, 1550] and step=413, but the range is not divisible by `step`. It will be replaced by [310, 1549].\n",
      "  warnings.warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/optuna/distributions.py:716: UserWarning: The distribution is specified by [460, 2300] and step=613, but the range is not divisible by `step`. It will be replaced by [460, 2299].\n",
      "  warnings.warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17096 features\n",
      "RemoveFeaturesLowMAE keeping 8548 features\n",
      "RemoveCorrelatedFeatures keeping 6985 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 300793 features\n",
      "RemoveFeaturesLowMAE keeping 30079 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:01<00:00, 49.90it/s]\n",
      "LR finder stopped early after 95 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7197b1b2-002a-4d8b-bd68-a3729ca86530.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_7197b1b2-002a-4d8b-bd68-a3729ca86530.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: tensorboard/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17141 features\n",
      "RemoveFeaturesLowMAE keeping 8570 features\n",
      "RemoveCorrelatedFeatures keeping 6886 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 300874 features\n",
      "RemoveFeaturesLowMAE keeping 30087 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  94%|█████████▍| 94/100 [00:01<00:00, 54.04it/s]\n",
      "LR finder stopped early after 94 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a1bb55e2-39d7-455d-8c7c-af665a4d6a17.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a1bb55e2-39d7-455d-8c7c-af665a4d6a17.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=35` reached.\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17109 features\n",
      "RemoveFeaturesLowMAE keeping 8554 features\n",
      "RemoveCorrelatedFeatures keeping 6967 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 300868 features\n",
      "RemoveFeaturesLowMAE keeping 30086 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:   8%|▊         | 8/100 [00:00<00:03, 23.45it/s]LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  20%|██        | 20/100 [00:00<00:01, 40.75it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  57%|█████▋    | 57/100 [00:01<00:00, 54.94it/s]GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Finding best initial lr:  93%|█████████▎| 93/100 [00:01<00:00, 50.58it/s]\n",
      "LR finder stopped early after 93 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6ee87ed9-2198-4401-b152-78cfb40ff45c.ckpt\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6ee87ed9-2198-4401-b152-78cfb40ff45c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=37` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=45` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17112 features\n",
      "RemoveFeaturesLowMAE keeping 8556 features\n",
      "RemoveCorrelatedFeatures keeping 7075 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 300520 features\n",
      "RemoveFeaturesLowMAE keeping 30052 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  94%|█████████▍| 94/100 [00:01<00:00, 62.11it/s]\n",
      "LR finder stopped early after 94 steps due to diverging loss.\n",
      "Learning rate set to 0.002754228703338169\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f2e60fab-14b4-4bee-a7f3-ff952f630956.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_f2e60fab-14b4-4bee-a7f3-ff952f630956.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=32` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:  10%|█         | 10/100 [00:00<00:00, 93.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17132 features\n",
      "RemoveFeaturesLowMAE keeping 8566 features\n",
      "RemoveCorrelatedFeatures keeping 6944 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 300756 features\n",
      "RemoveFeaturesLowMAE keeping 30075 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  94%|█████████▍| 94/100 [00:01<00:00, 89.52it/s]\n",
      "LR finder stopped early after 94 steps due to diverging loss.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_efbab9e9-4b38-40ae-910b-b0f86b376b4d.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_efbab9e9-4b38-40ae-910b-b0f86b376b4d.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=22` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17163 features\n",
      "RemoveFeaturesLowMAE keeping 8581 features\n",
      "RemoveCorrelatedFeatures keeping 6842 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 301047 features\n",
      "RemoveFeaturesLowMAE keeping 30104 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  96%|█████████▌| 96/100 [00:01<00:00, 52.71it/s]\n",
      "LR finder stopped early after 96 steps due to diverging loss.\n",
      "Learning rate set to 0.003311311214825908\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_07588522-90dd-4e98-8547-f635c1239a4a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_07588522-90dd-4e98-8547-f635c1239a4a.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=28` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17083 features\n",
      "RemoveFeaturesLowMAE keeping 8541 features\n",
      "RemoveCorrelatedFeatures keeping 6960 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 300916 features\n",
      "RemoveFeaturesLowMAE keeping 30091 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  94%|█████████▍| 94/100 [00:01<00:00, 53.33it/s]\n",
      "LR finder stopped early after 94 steps due to diverging loss.\n",
      "Learning rate set to 0.005754399373371567\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d07637c5-97f6-496c-9787-77ef8f02696c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_d07637c5-97f6-496c-9787-77ef8f02696c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   6%|▌         | 6/100 [00:00<00:01, 56.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17112 features\n",
      "RemoveFeaturesLowMAE keeping 8556 features\n",
      "RemoveCorrelatedFeatures keeping 6778 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 300957 features\n",
      "RemoveFeaturesLowMAE keeping 30095 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  96%|█████████▌| 96/100 [00:01<00:00, 57.13it/s]\n",
      "LR finder stopped early after 96 steps due to diverging loss.\n",
      "Learning rate set to 0.0009120108393559097\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6e80b935-b739-41da-a83b-caf4b3dec49a.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_6e80b935-b739-41da-a83b-caf4b3dec49a.ckpt\n",
      "`Trainer.fit` stopped: `max_epochs=40` reached.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17121 features\n",
      "RemoveFeaturesLowMAE keeping 8560 features\n",
      "RemoveCorrelatedFeatures keeping 6877 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 301043 features\n",
      "RemoveFeaturesLowMAE keeping 30104 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  94%|█████████▍| 94/100 [00:01<00:00, 48.49it/s]\n",
      "LR finder stopped early after 94 steps due to diverging loss.\n",
      "Learning rate set to 0.0013182567385564075\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_623c5ebe-5bd6-40d6-b84b-34208bce45ee.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_623c5ebe-5bd6-40d6-b84b-34208bce45ee.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=31` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=34` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17129 features\n",
      "RemoveFeaturesLowMAE keeping 8564 features\n",
      "RemoveCorrelatedFeatures keeping 6758 features\n",
      "FeatureSelectionNMF keeping 3100 features\n",
      "RemoveFeaturesWithNaN keeping 300998 features\n",
      "RemoveFeaturesLowMAE keeping 30099 features\n",
      "FeatureSelectionNMF keeping 4600 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  95%|█████████▌| 95/100 [00:01<00:00, 90.78it/s]\n",
      "LR finder stopped early after 95 steps due to diverging loss.\n",
      "Learning rate set to 0.000630957344480193\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5eddf809-b2c5-4c83-8f3f-cd10e5e04773.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_5eddf809-b2c5-4c83-8f3f-cd10e5e04773.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 3.6 M \n",
      "1 | decoder_0 | FCN  | 3.6 M \n",
      "2 | encoder_1 | FCN  | 10.7 M\n",
      "3 | decoder_1 | FCN  | 10.8 M\n",
      "-----------------------------------\n",
      "28.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.7 M    Total params\n",
      "114.951   Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=61` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\u001b[32m[I 2023-05-27 11:39:57,056]\u001b[0m Trial 0 pruned. \u001b[0m\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   9%|▉         | 9/100 [00:00<00:01, 87.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17096 features\n",
      "RemoveFeaturesLowMAE keeping 8548 features\n",
      "RemoveCorrelatedFeatures keeping 6985 features\n",
      "FeatureSelectionNMF keeping 4200 features\n",
      "RemoveFeaturesWithNaN keeping 300793 features\n",
      "RemoveFeaturesLowMAE keeping 30079 features\n",
      "FeatureSelectionNMF keeping 3300 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 94.01it/s] \n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.003981071705534969\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a3497c37-2e31-4874-94fe-01bbc93c779c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a3497c37-2e31-4874-94fe-01bbc93c779c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 1.8 M \n",
      "1 | decoder_0 | FCN  | 1.9 M \n",
      "2 | encoder_1 | FCN  | 1.2 M \n",
      "3 | decoder_1 | FCN  | 1.2 M \n",
      "-----------------------------------\n",
      "6.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.0 M     Total params\n",
      "24.025    Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=25` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17141 features\n",
      "RemoveFeaturesLowMAE keeping 8570 features\n",
      "RemoveCorrelatedFeatures keeping 6886 features\n",
      "FeatureSelectionNMF keeping 4200 features\n",
      "RemoveFeaturesWithNaN keeping 300874 features\n",
      "RemoveFeaturesLowMAE keeping 30087 features\n",
      "FeatureSelectionNMF keeping 3300 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:01<00:00, 52.43it/s]\n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0022908676527677745\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a56c010f-71f8-46dd-bb39-102f59dd8db1.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a56c010f-71f8-46dd-bb39-102f59dd8db1.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 1.8 M \n",
      "1 | decoder_0 | FCN  | 1.9 M \n",
      "2 | encoder_1 | FCN  | 1.2 M \n",
      "3 | decoder_1 | FCN  | 1.2 M \n",
      "-----------------------------------\n",
      "6.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.0 M     Total params\n",
      "24.025    Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=27` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=3` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17109 features\n",
      "RemoveFeaturesLowMAE keeping 8554 features\n",
      "RemoveCorrelatedFeatures keeping 6967 features\n",
      "FeatureSelectionNMF keeping 4200 features\n",
      "RemoveFeaturesWithNaN keeping 300868 features\n",
      "RemoveFeaturesLowMAE keeping 30086 features\n",
      "FeatureSelectionNMF keeping 3300 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  98%|█████████▊| 98/100 [00:01<00:00, 59.73it/s]\n",
      "LR finder stopped early after 98 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_95b81dc4-6512-4866-b238-5236aaf12486.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_95b81dc4-6512-4866-b238-5236aaf12486.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 1.8 M \n",
      "1 | decoder_0 | FCN  | 1.9 M \n",
      "2 | encoder_1 | FCN  | 1.2 M \n",
      "3 | decoder_1 | FCN  | 1.2 M \n",
      "-----------------------------------\n",
      "6.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.0 M     Total params\n",
      "24.025    Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=30` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=2` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: PossibleUserWarning: You defined a `validation_step` but have no `val_dataloader`. Skipping val loop.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RemoveFeaturesWithZeros keeping 17112 features\n",
      "RemoveFeaturesLowMAE keeping 8556 features\n",
      "RemoveCorrelatedFeatures keeping 7075 features\n",
      "FeatureSelectionNMF keeping 4200 features\n",
      "RemoveFeaturesWithNaN keeping 300520 features\n",
      "RemoveFeaturesLowMAE keeping 30052 features\n",
      "FeatureSelectionNMF keeping 3300 features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding best initial lr:  99%|█████████▉| 99/100 [00:01<00:00, 86.45it/s] \n",
      "LR finder stopped early after 99 steps due to diverging loss.\n",
      "Learning rate set to 0.0019054607179632484\n",
      "Restoring states from the checkpoint path at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a439da51-8702-4502-8d16-bd0034f9772c.ckpt\n",
      "Restored all states from the checkpoint at /home/alberto/Work/course_interpretability_deep_learning/.lr_find_a439da51-8702-4502-8d16-bd0034f9772c.ckpt\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type | Params\n",
      "-----------------------------------\n",
      "0 | encoder_0 | FCN  | 1.8 M \n",
      "1 | decoder_0 | FCN  | 1.9 M \n",
      "2 | encoder_1 | FCN  | 1.2 M \n",
      "3 | decoder_1 | FCN  | 1.2 M \n",
      "-----------------------------------\n",
      "6.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.0 M     Total params\n",
      "24.025    Total estimated model params size (MB)\n",
      "`Trainer.fit` stopped: `max_epochs=24` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/loops/utilities.py:70: PossibleUserWarning: `max_epochs` was not set. Setting it to 1000 epochs. To train without an epoch limit, set `max_epochs=-1`.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 20 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "`Trainer.fit` stopped: `max_epochs=14` reached.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/alberto/anaconda3/envs/course_interpretability_dl/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:478: PossibleUserWarning: Your `val_dataloader`'s sampler has shuffling enabled, it is strongly recommended that you turn shuffling off for val/test dataloaders.\n",
      "  rank_zero_warn(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "study = Optimization.optimize_optuna_and_save(study=study, n_trials = 100, date= time.strftime('%Y%m%d%H'), show_progress_bar= True,\n",
    "                                              folder= settings.RESULTS_FOLDER, func= func_objective)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
